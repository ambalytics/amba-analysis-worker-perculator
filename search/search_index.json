{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"amba-analysis-worker-perculator The percolator component connects a discussion event, i.e., a tweet, to a publication or at least a DOI. Without the connection, the event can not be further processed by the analytical components. The percolator is based on ideas developed by CrossRef, that use a percolator for linking events as well. The percolator is developed in python using the amba-event-stream package. It runs as a docker container. In order to improve linking throughput, three processes run simultaneously in the percolator to reduce the time overhead generated by waiting on web responses. Since all processes will connect to Kafka by using the same consumer ID, identifying them to be the same type, Kafka automatically shares the events equally between the 3 processes. A DOI resolver class is used, defining static functions to retrieve the DOI from event data. One way of linking data is using Meta Tags, these are HTML tags embedded in the source code of a webpage. These tags will not be displayed by a browser and are used to specify information to automatic systems processing the page. Traditionally, these have been used by Search Engines and are nowadays additionally used to provide data for custom titles, descriptions and all kinds of crawler information. Each meta tag has a name attribute as well as a content attribute. The name is used to identify what metadata is stored, the content will contain the actual value. This allows to filter the tags for only relevant data which can easily be identified without the need of analyzing the content. This ensures correct data compared to a full-text analysis, which may result in wrong results since no context analysis is done. For Example, a citation may be found using the full-text analysis but wrongly be used to link the event. Multiple methods of extraction are used to find the DOI for given discussion events. An overview of the process can be seen in Figure 1. First, the tweet data is checked for URLs. If that fails or these URLs do not contain a DOI, additionally, all referenced tweets will be checked. A response may not contain an URL itself but reference the original tweet the response is responding to. Thus, the URL needs to be linked. Note that multiple URLs are available from the Twitter API, that differentiate in their characteristics: a short URL, an expanded URL as well as an unwound URL. Since the DOI ideally can be extracted from the URL itself, the expanded and unwound URL are both checked. Sometimes only the expanded URL contains the DOI, while otherwhile the unwound URL does. Processing Schema of Linking a Tweet with a Publication DOI The function of linking an URL with a DOI is cached up to 10000 URLs by a Least recently used (LRU) cache. Caching this function allows for very little storage needed to cache since function parameter and result are both small. The cache generally allows faster processing and less need for requests in general. The LRU caching strategy ensures the most used are staying in the cache. The data is static and likely to not change ensuring time is not relevant for cache to expire. A URL to be linked is first checked with multiple regex to extract potential DOIs. These regex are based on CrossRef but extended to suit the publisher URLs registered in the system. Since the DOI specification is not extremely strict, there needs to be verification of a potential DOI. To confirmation that a DOI exists, a check is done by sending a request to doi.org. Due to DOI specification, an existing DOI is linking to the correct publication. If no DOI can be extracted using the regex on the URL, a request is sent to the CrossRef event API. The event rest API of CrossRef allows checking if they already linked a URL to a publication. If again no DOI can be retrieved, a request using the URL is sent. The response is subsequently checked for a set list of meta tags. If one or more such tags are found, their values are extracted and a DOI linking started on each of them. The last function if all other fail is to check the fulltext HTML for a DOI. All publisher URLs are checked to ensure their working in the system. However their articles and pages on their sites which are not registered with a DOI. This means the percolator will not be able to link tweets linking to these URLs. Once a DOI for a tweet is found, the database is queried for the publication data. If the data can be retrieved it will be added to the event, the event is then set to linked and published. Otherwise, it is set to unknown before sending it to Kafka. If no DOI can be found for a tweet, it\u2019s not further processed in the pipeline and the data will be lost.","title":"Home"},{"location":"#amba-analysis-worker-perculator","text":"The percolator component connects a discussion event, i.e., a tweet, to a publication or at least a DOI. Without the connection, the event can not be further processed by the analytical components. The percolator is based on ideas developed by CrossRef, that use a percolator for linking events as well. The percolator is developed in python using the amba-event-stream package. It runs as a docker container. In order to improve linking throughput, three processes run simultaneously in the percolator to reduce the time overhead generated by waiting on web responses. Since all processes will connect to Kafka by using the same consumer ID, identifying them to be the same type, Kafka automatically shares the events equally between the 3 processes. A DOI resolver class is used, defining static functions to retrieve the DOI from event data. One way of linking data is using Meta Tags, these are HTML tags embedded in the source code of a webpage. These tags will not be displayed by a browser and are used to specify information to automatic systems processing the page. Traditionally, these have been used by Search Engines and are nowadays additionally used to provide data for custom titles, descriptions and all kinds of crawler information. Each meta tag has a name attribute as well as a content attribute. The name is used to identify what metadata is stored, the content will contain the actual value. This allows to filter the tags for only relevant data which can easily be identified without the need of analyzing the content. This ensures correct data compared to a full-text analysis, which may result in wrong results since no context analysis is done. For Example, a citation may be found using the full-text analysis but wrongly be used to link the event. Multiple methods of extraction are used to find the DOI for given discussion events. An overview of the process can be seen in Figure 1. First, the tweet data is checked for URLs. If that fails or these URLs do not contain a DOI, additionally, all referenced tweets will be checked. A response may not contain an URL itself but reference the original tweet the response is responding to. Thus, the URL needs to be linked. Note that multiple URLs are available from the Twitter API, that differentiate in their characteristics: a short URL, an expanded URL as well as an unwound URL. Since the DOI ideally can be extracted from the URL itself, the expanded and unwound URL are both checked. Sometimes only the expanded URL contains the DOI, while otherwhile the unwound URL does. Processing Schema of Linking a Tweet with a Publication DOI The function of linking an URL with a DOI is cached up to 10000 URLs by a Least recently used (LRU) cache. Caching this function allows for very little storage needed to cache since function parameter and result are both small. The cache generally allows faster processing and less need for requests in general. The LRU caching strategy ensures the most used are staying in the cache. The data is static and likely to not change ensuring time is not relevant for cache to expire. A URL to be linked is first checked with multiple regex to extract potential DOIs. These regex are based on CrossRef but extended to suit the publisher URLs registered in the system. Since the DOI specification is not extremely strict, there needs to be verification of a potential DOI. To confirmation that a DOI exists, a check is done by sending a request to doi.org. Due to DOI specification, an existing DOI is linking to the correct publication. If no DOI can be extracted using the regex on the URL, a request is sent to the CrossRef event API. The event rest API of CrossRef allows checking if they already linked a URL to a publication. If again no DOI can be retrieved, a request using the URL is sent. The response is subsequently checked for a set list of meta tags. If one or more such tags are found, their values are extracted and a DOI linking started on each of them. The last function if all other fail is to check the fulltext HTML for a DOI. All publisher URLs are checked to ensure their working in the system. However their articles and pages on their sites which are not registered with a DOI. This means the percolator will not be able to link tweets linking to these URLs. Once a DOI for a tweet is found, the database is queried for the publication data. If the data can be retrieved it will be added to the event, the event is then set to linked and published. Otherwise, it is set to unknown before sending it to Kafka. If no DOI can be found for a tweet, it\u2019s not further processed in the pipeline and the data will be lost.","title":"amba-analysis-worker-perculator"},{"location":"doi_resolver_ref/","text":"Helper Class providing multiple static functions to extract a doi from a url check_doi_list_valid ( potential_dois ) check if a list of potential dois are valid and if so return the valid doi Parameters: Name Type Description Default potential_dois a list of dois that should be checked required Source code in src/doi_resolver.py def check_doi_list_valid ( potential_dois ): \"\"\"check if a list of potential dois are valid and if so return the valid doi Arguments: potential_dois: a list of dois that should be checked \"\"\" pdoi = '' for doi in potential_dois : if doi is not None and doi : pdoi = pdoi + doi + ',' pre = \"http://doi.org/doiRA/\" # todo rw = False if pdoi != '' : r = requests . get ( pre + pdoi ) json_response = r . json () for j in json_response : if 'RA' in j : rw = j [ 'DOI' ] return rw crossref_url_search ( url ) search the url in crossref eventdata to get a doi Parameters: Name Type Description Default url the url to get a doi for required Source code in src/doi_resolver.py def crossref_url_search ( url ): \"\"\"search the url in crossref eventdata to get a doi Arguments: url: the url to get a doi for \"\"\" r = requests . get ( \"https://api.eventdata.crossref.org/v1/events?rows=1&obj.url=\" + url ) if r . status_code == 200 : json_response = r . json () if 'status' in json_response : if json_response [ 'status' ] == 'ok' : # logging.debug(json_response) if 'message' in json_response : if 'events' in json_response [ 'message' ]: for event in json_response [ 'message' ][ 'events' ]: return event [ 'obj_id' ][ 16 :] # https://doi.org/ -> 16 return False get_dois_regex ( regex , temp_doi ) find all occurrences of a given regex in a doi-string which ending is searched Parameters: Name Type Description Default regex the regex to look for required temp_doi the doi to use required Source code in src/doi_resolver.py def get_dois_regex ( regex , temp_doi ): \"\"\"find all occurrences of a given regex in a doi-string which ending is searched Arguments: regex: the regex to look for temp_doi: the doi to use \"\"\" if regex . search ( temp_doi ) is not None : return regex . findall ( temp_doi )[ 0 ] get_filtered_dois_from_meta ( potential_dois ) check potential dois from meta tags for dois to extract them in case they have a full url Parameters: Name Type Description Default potential_dois the dois to filter required Source code in src/doi_resolver.py def get_filtered_dois_from_meta ( potential_dois ): \"\"\"check potential dois from meta tags for dois to extract them in case they have a full url Arguments: potential_dois: the dois to filter \"\"\" result = set ([]) for t in potential_dois : result . add ( t . replace ( 'doi:' , '' )) doi_re = re . compile ( \"(10 \\\\ . \\\\ d{4,9}(?:/| %2F | %2f )[^ \\\\ s]+)\" ) r = set ([]) for t in result : if doi_re . search ( t ) is not None : r . add ( t ) return r get_lxml ( page ) use lxml to search for meta tags that could contain the doi Parameters: Name Type Description Default page the page to search required Source code in src/doi_resolver.py def get_lxml ( page ): \"\"\"use lxml to search for meta tags that could contain the doi Arguments: page: the page to search \"\"\" content = html . fromstring ( page . content ) result = set ([]) for meta in content . xpath ( '//meta' ): for name , value in sorted ( meta . items ()): if value . strip () . lower () in [ 'citation_doi' , 'dc.identifier' , 'evt-doipage' , 'news_doi' , 'citation_doi' , 'doi' , 'DC.DOI' , 'DC.Identifier.DOI' , 'DOIs' , 'bepress_citation_doi' , 'rft_id' ]: result . add ( meta . get ( 'content' )) return result get_potential_dois_from_text ( text ) use multiple different regexes to get a list of potential dois, it uses a very generic regex first which only checks for the bare minimum start to the end of line, this result will than be searched for possible endings generating possible dois the result is a set and will likely contain unvalid dois Parameters: Name Type Description Default text the text which should be checked required Source code in src/doi_resolver.py def get_potential_dois_from_text ( text ): \"\"\"use multiple different regexes to get a list of potential dois, it uses a very generic regex first which only checks for the bare minimum start to the end of line, this result will than be searched for possible endings generating possible dois the result is a set and will likely contain unvalid dois Arguments: text: the text which should be checked \"\"\" doi_re = re . compile ( \"(10 \\\\ . \\\\ d{4,9}(?:/| %2F | %2f )[^ \\\\ s]+)\" ) last_slash = re . compile ( \"^(10 \\\\ . \\\\ d+/.*)/.*$\" ) first_slash = re . compile ( \"^(10 \\\\ . \\\\ d+/.*?)/.*$\" ) semicolon = re . compile ( \"^(10 \\\\ . \\\\ d+/.*);.*$\" ) hashchar = re . compile ( \"^(10 \\\\ . \\\\ d+/.*?)#.*$\" ) question_mark = re . compile ( \"^(10 \\\\ . \\\\ d+/.*?) \\\\ ?.*$\" ) amp_mark = re . compile ( \"^(10 \\\\ . \\\\ d+/.*?)&.*$\" ) v1 = re . compile ( \"^(10 \\\\ . \\\\ d+/.*)v1*$\" ) # biorxiv, make if v\\\\d+ (v and digit v1,v2,v3 result = set ([]) if doi_re . search ( text ) is not None : temp_doi = doi_re . search ( text ) . group () result . add ( temp_doi ) result . add ( get_dois_regex ( last_slash , temp_doi )) result . add ( get_dois_regex ( first_slash , temp_doi )) result . add ( get_dois_regex ( semicolon , temp_doi )) result . add ( get_dois_regex ( hashchar , temp_doi )) result . add ( get_dois_regex ( question_mark , temp_doi )) result . add ( get_dois_regex ( amp_mark , temp_doi )) result . add ( get_dois_regex ( v1 , temp_doi )) return result get_response ( url , s , r = 0 ) get a response from a given url using a given session s, a session can be used for headers, this function is cached up to 100 elements !!! arguments url: the url to get s: the session to use Source code in src/doi_resolver.py @lru_cache ( maxsize = 100 ) def get_response ( url , s , r = 0 ): \"\"\"get a response from a given url using a given session s, a session can be used for headers, this function is cached up to 100 elements Arguments: url: the url to get s: the session to use \"\"\" try : url = url . replace ( 'arxiv.org' , 'export.arxiv.org' ) # arxiv wants this url to be used by machines result = s . get ( url , stream = False , timeout = 5 ) except ( ConnectionRefusedError , SSLError , ReadTimeoutError , requests . exceptions . TooManyRedirects , requests . exceptions . ConnectionError , requests . exceptions . ReadTimeout , NewConnectionError , requests . exceptions . SSLError , ConnectionError ): logging . warning ( 'Percolator error, reset session' ) s = Session () # get the response for the provided url headers = { 'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.104 Safari/537.36' } s . headers . update ( headers ) if r < 3 : logging . warning ( 'retry in ' + str ( pow ( 2 , r )) + 's' ) time . sleep ( pow ( 2 , r )) get_response ( url , s , r + 1 ) else : return None else : return result return None link_url ( url ) link a url to a valid doi, it will try to get potential dois using multiple regex and than check if their are valid and than return the doi it uses multiple methods to search for the doi, this function is cached up to 10000 elements !!! arguments url: the url to get s: the session to use Source code in src/doi_resolver.py @lru_cache ( maxsize = 10000 ) def link_url ( url ): \"\"\"link a url to a valid doi, it will try to get potential dois using multiple regex and than check if their are valid and than return the doi it uses multiple methods to search for the doi, this function is cached up to 10000 elements Arguments: url: the url to get s: the session to use \"\"\" # logging.warning(url) # check if the url contains the doi doi = check_doi_list_valid ( get_potential_dois_from_text ( url )) if doi : logging . debug ( 'url' ) return doi s = Session () # get the response for the provided url headers = { 'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.104 Safari/537.36' } s . headers . update ( headers ) r = get_response ( url , s ) # check if the doi is in any meta tag if r : pot_doi = get_lxml ( r ) doi = check_doi_list_valid ( get_filtered_dois_from_meta ( pot_doi )) if doi and doi != set ([]): logging . debug ( 'meta' ) return doi # check if crossref knows this url and returns the doi doi = crossref_url_search ( url ) if doi : logging . debug ( 'crossref' ) return doi if r : # do a fulltext search of the url doi = check_doi_list_valid ( search_fulltext ( r )) if doi : logging . debug ( 'fulltext' ) return doi return False search_fulltext ( r ) search the fulltext of an response Parameters: Name Type Description Default r the response we want to search required Source code in src/doi_resolver.py def search_fulltext ( r ): \"\"\"search the fulltext of an response Arguments: r: the response we want to search \"\"\" return get_potential_dois_from_text ( r . text ) url_doi_check ( data ) check data for urls, get first url in urls, prefer expanded_url !!! arguments data: data to get url from Source code in src/doi_resolver.py def url_doi_check ( data ): \"\"\"check data for urls, get first url in urls, prefer expanded_url Arguments: data: data to get url from \"\"\" doi_data = False if 'entities' in data : if 'urls' in data [ 'entities' ]: for url in data [ 'entities' ][ 'urls' ]: if doi_data is False and 'expanded_url' in url : doi_data = link_url ( url [ 'expanded_url' ]) if doi_data is False and 'unwound_url' in url : doi_data = link_url ( url [ 'unwound_url' ]) if doi_data is not False : logging . debug ( doi_data ) return doi_data if doi_data is not False : logging . debug ( doi_data ) return doi_data return doi_data","title":"doi resolver"},{"location":"doi_resolver_ref/#doi_resolver.check_doi_list_valid","text":"check if a list of potential dois are valid and if so return the valid doi Parameters: Name Type Description Default potential_dois a list of dois that should be checked required Source code in src/doi_resolver.py def check_doi_list_valid ( potential_dois ): \"\"\"check if a list of potential dois are valid and if so return the valid doi Arguments: potential_dois: a list of dois that should be checked \"\"\" pdoi = '' for doi in potential_dois : if doi is not None and doi : pdoi = pdoi + doi + ',' pre = \"http://doi.org/doiRA/\" # todo rw = False if pdoi != '' : r = requests . get ( pre + pdoi ) json_response = r . json () for j in json_response : if 'RA' in j : rw = j [ 'DOI' ] return rw","title":"check_doi_list_valid()"},{"location":"doi_resolver_ref/#doi_resolver.crossref_url_search","text":"search the url in crossref eventdata to get a doi Parameters: Name Type Description Default url the url to get a doi for required Source code in src/doi_resolver.py def crossref_url_search ( url ): \"\"\"search the url in crossref eventdata to get a doi Arguments: url: the url to get a doi for \"\"\" r = requests . get ( \"https://api.eventdata.crossref.org/v1/events?rows=1&obj.url=\" + url ) if r . status_code == 200 : json_response = r . json () if 'status' in json_response : if json_response [ 'status' ] == 'ok' : # logging.debug(json_response) if 'message' in json_response : if 'events' in json_response [ 'message' ]: for event in json_response [ 'message' ][ 'events' ]: return event [ 'obj_id' ][ 16 :] # https://doi.org/ -> 16 return False","title":"crossref_url_search()"},{"location":"doi_resolver_ref/#doi_resolver.get_dois_regex","text":"find all occurrences of a given regex in a doi-string which ending is searched Parameters: Name Type Description Default regex the regex to look for required temp_doi the doi to use required Source code in src/doi_resolver.py def get_dois_regex ( regex , temp_doi ): \"\"\"find all occurrences of a given regex in a doi-string which ending is searched Arguments: regex: the regex to look for temp_doi: the doi to use \"\"\" if regex . search ( temp_doi ) is not None : return regex . findall ( temp_doi )[ 0 ]","title":"get_dois_regex()"},{"location":"doi_resolver_ref/#doi_resolver.get_filtered_dois_from_meta","text":"check potential dois from meta tags for dois to extract them in case they have a full url Parameters: Name Type Description Default potential_dois the dois to filter required Source code in src/doi_resolver.py def get_filtered_dois_from_meta ( potential_dois ): \"\"\"check potential dois from meta tags for dois to extract them in case they have a full url Arguments: potential_dois: the dois to filter \"\"\" result = set ([]) for t in potential_dois : result . add ( t . replace ( 'doi:' , '' )) doi_re = re . compile ( \"(10 \\\\ . \\\\ d{4,9}(?:/| %2F | %2f )[^ \\\\ s]+)\" ) r = set ([]) for t in result : if doi_re . search ( t ) is not None : r . add ( t ) return r","title":"get_filtered_dois_from_meta()"},{"location":"doi_resolver_ref/#doi_resolver.get_lxml","text":"use lxml to search for meta tags that could contain the doi Parameters: Name Type Description Default page the page to search required Source code in src/doi_resolver.py def get_lxml ( page ): \"\"\"use lxml to search for meta tags that could contain the doi Arguments: page: the page to search \"\"\" content = html . fromstring ( page . content ) result = set ([]) for meta in content . xpath ( '//meta' ): for name , value in sorted ( meta . items ()): if value . strip () . lower () in [ 'citation_doi' , 'dc.identifier' , 'evt-doipage' , 'news_doi' , 'citation_doi' , 'doi' , 'DC.DOI' , 'DC.Identifier.DOI' , 'DOIs' , 'bepress_citation_doi' , 'rft_id' ]: result . add ( meta . get ( 'content' )) return result","title":"get_lxml()"},{"location":"doi_resolver_ref/#doi_resolver.get_potential_dois_from_text","text":"use multiple different regexes to get a list of potential dois, it uses a very generic regex first which only checks for the bare minimum start to the end of line, this result will than be searched for possible endings generating possible dois the result is a set and will likely contain unvalid dois Parameters: Name Type Description Default text the text which should be checked required Source code in src/doi_resolver.py def get_potential_dois_from_text ( text ): \"\"\"use multiple different regexes to get a list of potential dois, it uses a very generic regex first which only checks for the bare minimum start to the end of line, this result will than be searched for possible endings generating possible dois the result is a set and will likely contain unvalid dois Arguments: text: the text which should be checked \"\"\" doi_re = re . compile ( \"(10 \\\\ . \\\\ d{4,9}(?:/| %2F | %2f )[^ \\\\ s]+)\" ) last_slash = re . compile ( \"^(10 \\\\ . \\\\ d+/.*)/.*$\" ) first_slash = re . compile ( \"^(10 \\\\ . \\\\ d+/.*?)/.*$\" ) semicolon = re . compile ( \"^(10 \\\\ . \\\\ d+/.*);.*$\" ) hashchar = re . compile ( \"^(10 \\\\ . \\\\ d+/.*?)#.*$\" ) question_mark = re . compile ( \"^(10 \\\\ . \\\\ d+/.*?) \\\\ ?.*$\" ) amp_mark = re . compile ( \"^(10 \\\\ . \\\\ d+/.*?)&.*$\" ) v1 = re . compile ( \"^(10 \\\\ . \\\\ d+/.*)v1*$\" ) # biorxiv, make if v\\\\d+ (v and digit v1,v2,v3 result = set ([]) if doi_re . search ( text ) is not None : temp_doi = doi_re . search ( text ) . group () result . add ( temp_doi ) result . add ( get_dois_regex ( last_slash , temp_doi )) result . add ( get_dois_regex ( first_slash , temp_doi )) result . add ( get_dois_regex ( semicolon , temp_doi )) result . add ( get_dois_regex ( hashchar , temp_doi )) result . add ( get_dois_regex ( question_mark , temp_doi )) result . add ( get_dois_regex ( amp_mark , temp_doi )) result . add ( get_dois_regex ( v1 , temp_doi )) return result","title":"get_potential_dois_from_text()"},{"location":"doi_resolver_ref/#doi_resolver.get_response","text":"get a response from a given url using a given session s, a session can be used for headers, this function is cached up to 100 elements !!! arguments url: the url to get s: the session to use Source code in src/doi_resolver.py @lru_cache ( maxsize = 100 ) def get_response ( url , s , r = 0 ): \"\"\"get a response from a given url using a given session s, a session can be used for headers, this function is cached up to 100 elements Arguments: url: the url to get s: the session to use \"\"\" try : url = url . replace ( 'arxiv.org' , 'export.arxiv.org' ) # arxiv wants this url to be used by machines result = s . get ( url , stream = False , timeout = 5 ) except ( ConnectionRefusedError , SSLError , ReadTimeoutError , requests . exceptions . TooManyRedirects , requests . exceptions . ConnectionError , requests . exceptions . ReadTimeout , NewConnectionError , requests . exceptions . SSLError , ConnectionError ): logging . warning ( 'Percolator error, reset session' ) s = Session () # get the response for the provided url headers = { 'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.104 Safari/537.36' } s . headers . update ( headers ) if r < 3 : logging . warning ( 'retry in ' + str ( pow ( 2 , r )) + 's' ) time . sleep ( pow ( 2 , r )) get_response ( url , s , r + 1 ) else : return None else : return result return None","title":"get_response()"},{"location":"doi_resolver_ref/#doi_resolver.link_url","text":"link a url to a valid doi, it will try to get potential dois using multiple regex and than check if their are valid and than return the doi it uses multiple methods to search for the doi, this function is cached up to 10000 elements !!! arguments url: the url to get s: the session to use Source code in src/doi_resolver.py @lru_cache ( maxsize = 10000 ) def link_url ( url ): \"\"\"link a url to a valid doi, it will try to get potential dois using multiple regex and than check if their are valid and than return the doi it uses multiple methods to search for the doi, this function is cached up to 10000 elements Arguments: url: the url to get s: the session to use \"\"\" # logging.warning(url) # check if the url contains the doi doi = check_doi_list_valid ( get_potential_dois_from_text ( url )) if doi : logging . debug ( 'url' ) return doi s = Session () # get the response for the provided url headers = { 'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.104 Safari/537.36' } s . headers . update ( headers ) r = get_response ( url , s ) # check if the doi is in any meta tag if r : pot_doi = get_lxml ( r ) doi = check_doi_list_valid ( get_filtered_dois_from_meta ( pot_doi )) if doi and doi != set ([]): logging . debug ( 'meta' ) return doi # check if crossref knows this url and returns the doi doi = crossref_url_search ( url ) if doi : logging . debug ( 'crossref' ) return doi if r : # do a fulltext search of the url doi = check_doi_list_valid ( search_fulltext ( r )) if doi : logging . debug ( 'fulltext' ) return doi return False","title":"link_url()"},{"location":"doi_resolver_ref/#doi_resolver.search_fulltext","text":"search the fulltext of an response Parameters: Name Type Description Default r the response we want to search required Source code in src/doi_resolver.py def search_fulltext ( r ): \"\"\"search the fulltext of an response Arguments: r: the response we want to search \"\"\" return get_potential_dois_from_text ( r . text )","title":"search_fulltext()"},{"location":"doi_resolver_ref/#doi_resolver.url_doi_check","text":"check data for urls, get first url in urls, prefer expanded_url !!! arguments data: data to get url from Source code in src/doi_resolver.py def url_doi_check ( data ): \"\"\"check data for urls, get first url in urls, prefer expanded_url Arguments: data: data to get url from \"\"\" doi_data = False if 'entities' in data : if 'urls' in data [ 'entities' ]: for url in data [ 'entities' ][ 'urls' ]: if doi_data is False and 'expanded_url' in url : doi_data = link_url ( url [ 'expanded_url' ]) if doi_data is False and 'unwound_url' in url : doi_data = link_url ( url [ 'unwound_url' ]) if doi_data is not False : logging . debug ( doi_data ) return doi_data if doi_data is not False : logging . debug ( doi_data ) return doi_data return doi_data","title":"url_doi_check()"},{"location":"twitter_perculator_ref/","text":"TwitterPerculator ( EventStreamConsumer , EventStreamProducer ) link events to doi and if possible add a publication to it add_publication ( self , event , publication ) add a publication to an event Parameters: Name Type Description Default event the event we wan't to add a publication to required publication the publication to add required Source code in src/twitter_perculator.py def add_publication ( self , event , publication ): \"\"\"add a publication to an event Arguments: event: the event we wan't to add a publication to publication: the publication to add \"\"\" logging . debug ( self . log + \"linked publication\" ) event . data [ 'obj' ][ 'data' ] = publication doi_base_url = \"https://doi.org/\" # todo event . data [ 'obj' ][ 'pid' ] = doi_base_url + publication [ 'doi' ] event . data [ 'obj' ][ 'alternative-id' ] = publication [ 'doi' ] event . set ( 'obj_id' , event . data [ 'obj' ][ 'pid' ]) get_publication_info ( self , doi ) get publication data for a doi using mongo and amba dbs Parameters: Name Type Description Default doi the doi for the publication we want required Source code in src/twitter_perculator.py def get_publication_info ( self , doi ): \"\"\"get publication data for a doi using mongo and amba dbs Arguments: doi: the doi for the publication we want \"\"\" publication = self . dao . get_publication ( doi ) if publication and isinstance ( publication , dict ): logging . debug ( publication ) logging . debug ( 'get publication from db' ) return publication if publication : logging . debug ( 'get publication from amba' ) publication = self . dao . save_publication ( publication ) return publication return { 'doi' : doi } on_message ( self , json_msg ) either link a event to a publication or add doi to it and mark it unknown to add the publication finder topic Parameters: Name Type Description Default json_msg json message representing a event required Source code in src/twitter_perculator.py def on_message ( self , json_msg ): \"\"\"either link a event to a publication or add doi to it and mark it unknown to add the publication finder topic Arguments: json_msg: json message representing a event \"\"\" if not self . dao : self . dao = DAO () e = Event () e . from_json ( json_msg ) e . data [ 'obj' ][ 'data' ] = {} if e . get ( 'source_id' ) == 'twitter' : # logging.warning(self.log + \"on message twitter perculator\") if 'id' in e . data [ 'subj' ][ 'data' ]: # logging.warning(self.log + e.data['subj']['data']['id']) # we use the id for mongo todo e . data [ 'subj' ][ 'data' ][ '_id' ] = e . data [ 'subj' ][ 'data' ] . pop ( 'id' ) threading . Timer ( 120 , self . alive , args = [ e . data [ 'subj' ][ 'data' ][ '_id' ]]) . start () self . current_id = e . data [ 'subj' ][ 'data' ][ '_id' ] # logging.warning('start: ' + str(self.current_id)) # move matching rules to tweet self e . data [ 'subj' ][ 'data' ][ 'matching_rules' ] = e . data [ 'subj' ][ 'data' ][ 'matching_rules' ] # check for doi recognition on tweet self doi = doi_resolver . url_doi_check ( e . data [ 'subj' ][ 'data' ]) if doi is not False : self . update_event ( e , doi ) # check if we have the conversation_id in our db # where discussionData.subjId == conversation_id # check the includes object for the original tweet url elif 'tweets' in e . data [ 'subj' ][ 'data' ][ 'includes' ]: # logging.warning('tweets') for tweet in e . data [ 'subj' ][ 'data' ][ 'includes' ][ 'tweets' ]: doi = doi_resolver . url_doi_check ( tweet ) # logging.warning('doi 2 ' + str(doi)) if doi is not False : # use first doi we get self . update_event ( e , doi ) break if doi is False : logging . warning ( \"no doi\" ) logging . debug ( e . data [ 'subj' ][ 'data' ][ 'includes' ][ 'tweets' ]) else : logging . warning ( \"no doi\" ) logging . debug ( e . data [ 'subj' ][ 'data' ]) else : logging . warning ( 'no id' ) else : logging . warning ( 'not twitter' ) start ( i = 0 ) staticmethod start the consumer Source code in src/twitter_perculator.py @staticmethod def start ( i = 0 ): \"\"\"start the consumer \"\"\" tp = TwitterPerculator ( i ) logging . warning ( TwitterPerculator . log + 'Start %s ' % str ( i )) tp . consume () update_event ( self , event , doi ) update the event either with publication or just with doi and set the state accordingly Source code in src/twitter_perculator.py def update_event ( self , event , doi ): \"\"\"update the event either with publication or just with doi and set the state accordingly \"\"\" self . current_id = 1 event . data [ 'obj' ][ 'data' ][ 'doi' ] = doi publication = self . get_publication_info ( doi ) if publication and isinstance ( publication , dict ) and 'title' in publication : self . add_publication ( event , publication ) event . set ( 'state' , 'linked' ) else : self . add_publication ( event , { 'doi' : doi }) event . set ( 'state' , 'unknown' ) logging . warning ( event . get ( 'state' )) self . publish ( event )","title":"twitter perculator"},{"location":"twitter_perculator_ref/#twitter_perculator.TwitterPerculator","text":"link events to doi and if possible add a publication to it","title":"TwitterPerculator"},{"location":"twitter_perculator_ref/#twitter_perculator.TwitterPerculator.add_publication","text":"add a publication to an event Parameters: Name Type Description Default event the event we wan't to add a publication to required publication the publication to add required Source code in src/twitter_perculator.py def add_publication ( self , event , publication ): \"\"\"add a publication to an event Arguments: event: the event we wan't to add a publication to publication: the publication to add \"\"\" logging . debug ( self . log + \"linked publication\" ) event . data [ 'obj' ][ 'data' ] = publication doi_base_url = \"https://doi.org/\" # todo event . data [ 'obj' ][ 'pid' ] = doi_base_url + publication [ 'doi' ] event . data [ 'obj' ][ 'alternative-id' ] = publication [ 'doi' ] event . set ( 'obj_id' , event . data [ 'obj' ][ 'pid' ])","title":"add_publication()"},{"location":"twitter_perculator_ref/#twitter_perculator.TwitterPerculator.get_publication_info","text":"get publication data for a doi using mongo and amba dbs Parameters: Name Type Description Default doi the doi for the publication we want required Source code in src/twitter_perculator.py def get_publication_info ( self , doi ): \"\"\"get publication data for a doi using mongo and amba dbs Arguments: doi: the doi for the publication we want \"\"\" publication = self . dao . get_publication ( doi ) if publication and isinstance ( publication , dict ): logging . debug ( publication ) logging . debug ( 'get publication from db' ) return publication if publication : logging . debug ( 'get publication from amba' ) publication = self . dao . save_publication ( publication ) return publication return { 'doi' : doi }","title":"get_publication_info()"},{"location":"twitter_perculator_ref/#twitter_perculator.TwitterPerculator.on_message","text":"either link a event to a publication or add doi to it and mark it unknown to add the publication finder topic Parameters: Name Type Description Default json_msg json message representing a event required Source code in src/twitter_perculator.py def on_message ( self , json_msg ): \"\"\"either link a event to a publication or add doi to it and mark it unknown to add the publication finder topic Arguments: json_msg: json message representing a event \"\"\" if not self . dao : self . dao = DAO () e = Event () e . from_json ( json_msg ) e . data [ 'obj' ][ 'data' ] = {} if e . get ( 'source_id' ) == 'twitter' : # logging.warning(self.log + \"on message twitter perculator\") if 'id' in e . data [ 'subj' ][ 'data' ]: # logging.warning(self.log + e.data['subj']['data']['id']) # we use the id for mongo todo e . data [ 'subj' ][ 'data' ][ '_id' ] = e . data [ 'subj' ][ 'data' ] . pop ( 'id' ) threading . Timer ( 120 , self . alive , args = [ e . data [ 'subj' ][ 'data' ][ '_id' ]]) . start () self . current_id = e . data [ 'subj' ][ 'data' ][ '_id' ] # logging.warning('start: ' + str(self.current_id)) # move matching rules to tweet self e . data [ 'subj' ][ 'data' ][ 'matching_rules' ] = e . data [ 'subj' ][ 'data' ][ 'matching_rules' ] # check for doi recognition on tweet self doi = doi_resolver . url_doi_check ( e . data [ 'subj' ][ 'data' ]) if doi is not False : self . update_event ( e , doi ) # check if we have the conversation_id in our db # where discussionData.subjId == conversation_id # check the includes object for the original tweet url elif 'tweets' in e . data [ 'subj' ][ 'data' ][ 'includes' ]: # logging.warning('tweets') for tweet in e . data [ 'subj' ][ 'data' ][ 'includes' ][ 'tweets' ]: doi = doi_resolver . url_doi_check ( tweet ) # logging.warning('doi 2 ' + str(doi)) if doi is not False : # use first doi we get self . update_event ( e , doi ) break if doi is False : logging . warning ( \"no doi\" ) logging . debug ( e . data [ 'subj' ][ 'data' ][ 'includes' ][ 'tweets' ]) else : logging . warning ( \"no doi\" ) logging . debug ( e . data [ 'subj' ][ 'data' ]) else : logging . warning ( 'no id' ) else : logging . warning ( 'not twitter' )","title":"on_message()"},{"location":"twitter_perculator_ref/#twitter_perculator.TwitterPerculator.start","text":"start the consumer Source code in src/twitter_perculator.py @staticmethod def start ( i = 0 ): \"\"\"start the consumer \"\"\" tp = TwitterPerculator ( i ) logging . warning ( TwitterPerculator . log + 'Start %s ' % str ( i )) tp . consume ()","title":"start()"},{"location":"twitter_perculator_ref/#twitter_perculator.TwitterPerculator.update_event","text":"update the event either with publication or just with doi and set the state accordingly Source code in src/twitter_perculator.py def update_event ( self , event , doi ): \"\"\"update the event either with publication or just with doi and set the state accordingly \"\"\" self . current_id = 1 event . data [ 'obj' ][ 'data' ][ 'doi' ] = doi publication = self . get_publication_info ( doi ) if publication and isinstance ( publication , dict ) and 'title' in publication : self . add_publication ( event , publication ) event . set ( 'state' , 'linked' ) else : self . add_publication ( event , { 'doi' : doi }) event . set ( 'state' , 'unknown' ) logging . warning ( event . get ( 'state' )) self . publish ( event )","title":"update_event()"}]}