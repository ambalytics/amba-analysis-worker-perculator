{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"amba-analysis-worker-perculator Tries to map events of amba-analysis-stream to publications","title":"Home"},{"location":"#amba-analysis-worker-perculator","text":"Tries to map events of amba-analysis-stream to publications","title":"amba-analysis-worker-perculator"},{"location":"doi_resolver_ref/","text":"Helper Class providing multiple static functions to extract a doi from a url check_doi_list_valid ( potential_dois ) check if a list of potential dois are valid and if so return the valid doi Parameters: Name Type Description Default potential_dois a list of dois that should be checked required Source code in src/doi_resolver.py def check_doi_list_valid ( potential_dois ): \"\"\"check if a list of potential dois are valid and if so return the valid doi Arguments: potential_dois: a list of dois that should be checked \"\"\" pdoi = '' for doi in potential_dois : if doi is not None and doi : pdoi = pdoi + doi + ',' pre = \"http://doi.org/doiRA/\" # todo rw = False if pdoi != '' : r = requests . get ( pre + pdoi ) json_response = r . json () for j in json_response : if 'RA' in j : rw = j [ 'DOI' ] return rw crossref_url_search ( url ) search the url in crossref eventdata to get a doi Parameters: Name Type Description Default url the url to get a doi for required Source code in src/doi_resolver.py def crossref_url_search ( url ): \"\"\"search the url in crossref eventdata to get a doi Arguments: url: the url to get a doi for \"\"\" r = requests . get ( \"https://api.eventdata.crossref.org/v1/events?rows=1&obj.url=\" + url ) if r . status_code == 200 : json_response = r . json () if 'status' in json_response : if json_response [ 'status' ] == 'ok' : # logging.debug(json_response) if 'message' in json_response : if 'events' in json_response [ 'message' ]: for event in json_response [ 'message' ][ 'events' ]: return event [ 'obj_id' ][ 16 :] # https://doi.org/ -> 16 return False get_dois_regex ( regex , temp_doi ) find all occurrences of a given regex in a doi-string which ending is searched Parameters: Name Type Description Default regex the regex to look for required temp_doi the doi to use required Source code in src/doi_resolver.py def get_dois_regex ( regex , temp_doi ): \"\"\"find all occurrences of a given regex in a doi-string which ending is searched Arguments: regex: the regex to look for temp_doi: the doi to use \"\"\" if regex . search ( temp_doi ) is not None : # logging.debug(regex.search(temp_doi).group()) # logging.debug(regex.findall(temp_doi)) return regex . findall ( temp_doi )[ 0 ] get_filtered_dois_from_meta ( potential_dois ) check potential dois from meta tags for dois to extract them in case they have a full url Parameters: Name Type Description Default potential_dois the dois to filter required Source code in src/doi_resolver.py def get_filtered_dois_from_meta ( potential_dois ): \"\"\"check potential dois from meta tags for dois to extract them in case they have a full url Arguments: potential_dois: the dois to filter \"\"\" result = set ([]) for t in potential_dois : result . add ( t . replace ( 'doi:' , '' )) doi_re = re . compile ( \"(10 \\\\ . \\\\ d{4,9}(?:/| %2F | %2f )[^ \\\\ s]+)\" ) r = set ([]) for t in result : if doi_re . search ( t ) is not None : r . add ( t ) return r get_lxml ( page ) use lxml to search for meta tags that could contain the doi Parameters: Name Type Description Default page the page to search required Source code in src/doi_resolver.py def get_lxml ( page ): \"\"\"use lxml to search for meta tags that could contain the doi Arguments: page: the page to search \"\"\" content = html . fromstring ( page . content ) result = set ([]) for meta in content . xpath ( '//meta' ): for name , value in sorted ( meta . items ()): if value . strip () . lower () in [ 'citation_doi' , 'dc.identifier' , 'evt-doipage' , 'news_doi' , 'citation_doi' ]: result . add ( meta . get ( 'content' )) return result get_potential_dois_from_text ( text ) use multiple different regexes to get a list of potential dois, it uses a very generic regex first which only checks for the bare minimum start to the end of line, this result will than be searched for possible endings generating possible dois the result is a set and will likely contain unvalid dois Parameters: Name Type Description Default text the text which should be checked required Source code in src/doi_resolver.py def get_potential_dois_from_text ( text ): \"\"\"use multiple different regexes to get a list of potential dois, it uses a very generic regex first which only checks for the bare minimum start to the end of line, this result will than be searched for possible endings generating possible dois the result is a set and will likely contain unvalid dois Arguments: text: the text which should be checked \"\"\" doi_re = re . compile ( \"(10 \\\\ . \\\\ d{4,9}(?:/| %2F | %2f )[^ \\\\ s]+)\" ) last_slash = re . compile ( \"^(10 \\\\ . \\\\ d+/.*)/.*$\" ) first_slash = re . compile ( \"^(10 \\\\ . \\\\ d+/.*?)/.*$\" ) semicolon = re . compile ( \"^(10 \\\\ . \\\\ d+/.*);.*$\" ) hashchar = re . compile ( \"^(10 \\\\ . \\\\ d+/.*?)#.*$\" ) question_mark = re . compile ( \"^(10 \\\\ . \\\\ d+/.*?) \\\\ ?.*$\" ) amp_mark = re . compile ( \"^(10 \\\\ . \\\\ d+/.*?)&.*$\" ) v1 = re . compile ( \"^(10 \\\\ . \\\\ d+/.*)v1*$\" ) # biorxiv, make if v\\\\d+ (v and digit v1,v2,v3 result = set ([]) if doi_re . search ( text ) is not None : temp_doi = doi_re . search ( text ) . group () # logging.debug(doi_re.findall(ur)) # logging.debug(temp_doi) result . add ( temp_doi ) result . add ( get_dois_regex ( last_slash , temp_doi )) result . add ( get_dois_regex ( first_slash , temp_doi )) result . add ( get_dois_regex ( semicolon , temp_doi )) result . add ( get_dois_regex ( hashchar , temp_doi )) result . add ( get_dois_regex ( question_mark , temp_doi )) result . add ( get_dois_regex ( amp_mark , temp_doi )) result . add ( get_dois_regex ( v1 , temp_doi )) return result get_response ( url , s , r = 0 ) get a response from a given url using a given session s, a session can be used for headers, this function is cached up to 100 elements !!! arguments url: the url to get s: the session to use Source code in src/doi_resolver.py @lru_cache ( maxsize = 100 ) def get_response ( url , s , r = 0 ): \"\"\"get a response from a given url using a given session s, a session can be used for headers, this function is cached up to 100 elements Arguments: url: the url to get s: the session to use \"\"\" try : result = s . get ( url , timeout = 10 ) except ( ConnectionRefusedError , SSLError , ReadTimeoutError , requests . exceptions . TooManyRedirects , requests . exceptions . ConnectionError , requests . exceptions . ReadTimeout , NewConnectionError , requests . exceptions . SSLError , ConnectionError ): logging . warning ( 'Perculator error, reset session' ) s = Session () # get the response for the provided url headers = { 'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' , 'Pragma' : 'no-cache' } s . headers . update ( headers ) if r < 5 : get_response ( url , s , r + 1 ) else : return result return None link_url ( url ) link a url to a valid doi, it will try to get potential dois using multiple regex and than check if their are valid and than return the doi it uses multiple methods to search for the doi, this function is cached up to 10000 elements !!! arguments url: the url to get s: the session to use Source code in src/doi_resolver.py @lru_cache ( maxsize = 10000 ) def link_url ( url ): \"\"\"link a url to a valid doi, it will try to get potential dois using multiple regex and than check if their are valid and than return the doi it uses multiple methods to search for the doi, this function is cached up to 10000 elements Arguments: url: the url to get s: the session to use \"\"\" logging . debug ( url ) # check if the url contains the doi doi = check_doi_list_valid ( get_potential_dois_from_text ( url )) if doi : logging . debug ( 'url' ) return doi s = Session () # get the response for the provided url headers = { 'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' , 'Pragma' : 'no-cache' } s . headers . update ( headers ) r = get_response ( url , s ) # check if the doi is in any meta tag if r : pot_doi = get_lxml ( r ) doi = check_doi_list_valid ( get_filtered_dois_from_meta ( pot_doi )) if doi and doi != set ([]): logging . debug ( 'meta' ) return doi # check if crossref knows this url and returns the doi doi = crossref_url_search ( url ) if doi : logging . debug ( 'crossref' ) return doi if r : # do a fulltext search of the url doi = check_doi_list_valid ( search_fulltext ( r )) if doi : logging . debug ( 'fulltext' ) return doi return False search_fulltext ( r ) search the fulltext of an response Parameters: Name Type Description Default r the response we want to search required Source code in src/doi_resolver.py def search_fulltext ( r ): \"\"\"search the fulltext of an response Arguments: r: the response we want to search \"\"\" return get_potential_dois_from_text ( r . text ) url_doi_check ( data ) check data for urls, get first url in urls, prefer expanded_url !!! arguments data: data to get url from Source code in src/doi_resolver.py def url_doi_check ( data ): \"\"\"check data for urls, get first url in urls, prefer expanded_url Arguments: data: data to get url from \"\"\" doi_data = False if 'entities' in data : if 'urls' in data [ 'entities' ]: for url in data [ 'entities' ][ 'urls' ]: if doi_data is False and 'expanded_url' in url : doi_data = link_url ( url [ 'expanded_url' ]) if doi_data is False and 'unwound_url' in url : doi_data = link_url ( url [ 'unwound_url' ]) if doi_data is not False : return doi_data return doi_data","title":"doi resolver"},{"location":"doi_resolver_ref/#doi_resolver.check_doi_list_valid","text":"check if a list of potential dois are valid and if so return the valid doi Parameters: Name Type Description Default potential_dois a list of dois that should be checked required Source code in src/doi_resolver.py def check_doi_list_valid ( potential_dois ): \"\"\"check if a list of potential dois are valid and if so return the valid doi Arguments: potential_dois: a list of dois that should be checked \"\"\" pdoi = '' for doi in potential_dois : if doi is not None and doi : pdoi = pdoi + doi + ',' pre = \"http://doi.org/doiRA/\" # todo rw = False if pdoi != '' : r = requests . get ( pre + pdoi ) json_response = r . json () for j in json_response : if 'RA' in j : rw = j [ 'DOI' ] return rw","title":"check_doi_list_valid()"},{"location":"doi_resolver_ref/#doi_resolver.crossref_url_search","text":"search the url in crossref eventdata to get a doi Parameters: Name Type Description Default url the url to get a doi for required Source code in src/doi_resolver.py def crossref_url_search ( url ): \"\"\"search the url in crossref eventdata to get a doi Arguments: url: the url to get a doi for \"\"\" r = requests . get ( \"https://api.eventdata.crossref.org/v1/events?rows=1&obj.url=\" + url ) if r . status_code == 200 : json_response = r . json () if 'status' in json_response : if json_response [ 'status' ] == 'ok' : # logging.debug(json_response) if 'message' in json_response : if 'events' in json_response [ 'message' ]: for event in json_response [ 'message' ][ 'events' ]: return event [ 'obj_id' ][ 16 :] # https://doi.org/ -> 16 return False","title":"crossref_url_search()"},{"location":"doi_resolver_ref/#doi_resolver.get_dois_regex","text":"find all occurrences of a given regex in a doi-string which ending is searched Parameters: Name Type Description Default regex the regex to look for required temp_doi the doi to use required Source code in src/doi_resolver.py def get_dois_regex ( regex , temp_doi ): \"\"\"find all occurrences of a given regex in a doi-string which ending is searched Arguments: regex: the regex to look for temp_doi: the doi to use \"\"\" if regex . search ( temp_doi ) is not None : # logging.debug(regex.search(temp_doi).group()) # logging.debug(regex.findall(temp_doi)) return regex . findall ( temp_doi )[ 0 ]","title":"get_dois_regex()"},{"location":"doi_resolver_ref/#doi_resolver.get_filtered_dois_from_meta","text":"check potential dois from meta tags for dois to extract them in case they have a full url Parameters: Name Type Description Default potential_dois the dois to filter required Source code in src/doi_resolver.py def get_filtered_dois_from_meta ( potential_dois ): \"\"\"check potential dois from meta tags for dois to extract them in case they have a full url Arguments: potential_dois: the dois to filter \"\"\" result = set ([]) for t in potential_dois : result . add ( t . replace ( 'doi:' , '' )) doi_re = re . compile ( \"(10 \\\\ . \\\\ d{4,9}(?:/| %2F | %2f )[^ \\\\ s]+)\" ) r = set ([]) for t in result : if doi_re . search ( t ) is not None : r . add ( t ) return r","title":"get_filtered_dois_from_meta()"},{"location":"doi_resolver_ref/#doi_resolver.get_lxml","text":"use lxml to search for meta tags that could contain the doi Parameters: Name Type Description Default page the page to search required Source code in src/doi_resolver.py def get_lxml ( page ): \"\"\"use lxml to search for meta tags that could contain the doi Arguments: page: the page to search \"\"\" content = html . fromstring ( page . content ) result = set ([]) for meta in content . xpath ( '//meta' ): for name , value in sorted ( meta . items ()): if value . strip () . lower () in [ 'citation_doi' , 'dc.identifier' , 'evt-doipage' , 'news_doi' , 'citation_doi' ]: result . add ( meta . get ( 'content' )) return result","title":"get_lxml()"},{"location":"doi_resolver_ref/#doi_resolver.get_potential_dois_from_text","text":"use multiple different regexes to get a list of potential dois, it uses a very generic regex first which only checks for the bare minimum start to the end of line, this result will than be searched for possible endings generating possible dois the result is a set and will likely contain unvalid dois Parameters: Name Type Description Default text the text which should be checked required Source code in src/doi_resolver.py def get_potential_dois_from_text ( text ): \"\"\"use multiple different regexes to get a list of potential dois, it uses a very generic regex first which only checks for the bare minimum start to the end of line, this result will than be searched for possible endings generating possible dois the result is a set and will likely contain unvalid dois Arguments: text: the text which should be checked \"\"\" doi_re = re . compile ( \"(10 \\\\ . \\\\ d{4,9}(?:/| %2F | %2f )[^ \\\\ s]+)\" ) last_slash = re . compile ( \"^(10 \\\\ . \\\\ d+/.*)/.*$\" ) first_slash = re . compile ( \"^(10 \\\\ . \\\\ d+/.*?)/.*$\" ) semicolon = re . compile ( \"^(10 \\\\ . \\\\ d+/.*);.*$\" ) hashchar = re . compile ( \"^(10 \\\\ . \\\\ d+/.*?)#.*$\" ) question_mark = re . compile ( \"^(10 \\\\ . \\\\ d+/.*?) \\\\ ?.*$\" ) amp_mark = re . compile ( \"^(10 \\\\ . \\\\ d+/.*?)&.*$\" ) v1 = re . compile ( \"^(10 \\\\ . \\\\ d+/.*)v1*$\" ) # biorxiv, make if v\\\\d+ (v and digit v1,v2,v3 result = set ([]) if doi_re . search ( text ) is not None : temp_doi = doi_re . search ( text ) . group () # logging.debug(doi_re.findall(ur)) # logging.debug(temp_doi) result . add ( temp_doi ) result . add ( get_dois_regex ( last_slash , temp_doi )) result . add ( get_dois_regex ( first_slash , temp_doi )) result . add ( get_dois_regex ( semicolon , temp_doi )) result . add ( get_dois_regex ( hashchar , temp_doi )) result . add ( get_dois_regex ( question_mark , temp_doi )) result . add ( get_dois_regex ( amp_mark , temp_doi )) result . add ( get_dois_regex ( v1 , temp_doi )) return result","title":"get_potential_dois_from_text()"},{"location":"doi_resolver_ref/#doi_resolver.get_response","text":"get a response from a given url using a given session s, a session can be used for headers, this function is cached up to 100 elements !!! arguments url: the url to get s: the session to use Source code in src/doi_resolver.py @lru_cache ( maxsize = 100 ) def get_response ( url , s , r = 0 ): \"\"\"get a response from a given url using a given session s, a session can be used for headers, this function is cached up to 100 elements Arguments: url: the url to get s: the session to use \"\"\" try : result = s . get ( url , timeout = 10 ) except ( ConnectionRefusedError , SSLError , ReadTimeoutError , requests . exceptions . TooManyRedirects , requests . exceptions . ConnectionError , requests . exceptions . ReadTimeout , NewConnectionError , requests . exceptions . SSLError , ConnectionError ): logging . warning ( 'Perculator error, reset session' ) s = Session () # get the response for the provided url headers = { 'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' , 'Pragma' : 'no-cache' } s . headers . update ( headers ) if r < 5 : get_response ( url , s , r + 1 ) else : return result return None","title":"get_response()"},{"location":"doi_resolver_ref/#doi_resolver.link_url","text":"link a url to a valid doi, it will try to get potential dois using multiple regex and than check if their are valid and than return the doi it uses multiple methods to search for the doi, this function is cached up to 10000 elements !!! arguments url: the url to get s: the session to use Source code in src/doi_resolver.py @lru_cache ( maxsize = 10000 ) def link_url ( url ): \"\"\"link a url to a valid doi, it will try to get potential dois using multiple regex and than check if their are valid and than return the doi it uses multiple methods to search for the doi, this function is cached up to 10000 elements Arguments: url: the url to get s: the session to use \"\"\" logging . debug ( url ) # check if the url contains the doi doi = check_doi_list_valid ( get_potential_dois_from_text ( url )) if doi : logging . debug ( 'url' ) return doi s = Session () # get the response for the provided url headers = { 'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' , 'Pragma' : 'no-cache' } s . headers . update ( headers ) r = get_response ( url , s ) # check if the doi is in any meta tag if r : pot_doi = get_lxml ( r ) doi = check_doi_list_valid ( get_filtered_dois_from_meta ( pot_doi )) if doi and doi != set ([]): logging . debug ( 'meta' ) return doi # check if crossref knows this url and returns the doi doi = crossref_url_search ( url ) if doi : logging . debug ( 'crossref' ) return doi if r : # do a fulltext search of the url doi = check_doi_list_valid ( search_fulltext ( r )) if doi : logging . debug ( 'fulltext' ) return doi return False","title":"link_url()"},{"location":"doi_resolver_ref/#doi_resolver.search_fulltext","text":"search the fulltext of an response Parameters: Name Type Description Default r the response we want to search required Source code in src/doi_resolver.py def search_fulltext ( r ): \"\"\"search the fulltext of an response Arguments: r: the response we want to search \"\"\" return get_potential_dois_from_text ( r . text )","title":"search_fulltext()"},{"location":"doi_resolver_ref/#doi_resolver.url_doi_check","text":"check data for urls, get first url in urls, prefer expanded_url !!! arguments data: data to get url from Source code in src/doi_resolver.py def url_doi_check ( data ): \"\"\"check data for urls, get first url in urls, prefer expanded_url Arguments: data: data to get url from \"\"\" doi_data = False if 'entities' in data : if 'urls' in data [ 'entities' ]: for url in data [ 'entities' ][ 'urls' ]: if doi_data is False and 'expanded_url' in url : doi_data = link_url ( url [ 'expanded_url' ]) if doi_data is False and 'unwound_url' in url : doi_data = link_url ( url [ 'unwound_url' ]) if doi_data is not False : return doi_data return doi_data","title":"url_doi_check()"},{"location":"twitter_perculator_ref/","text":"TwitterPerculator ( EventStreamConsumer , EventStreamProducer ) link events to doi and if possible add a publication to it add_publication ( self , event , publication ) add a publication to an event Parameters: Name Type Description Default event the event we wan't to add a publication to required publication the publication to add required Source code in src/twitter_perculator.py def add_publication ( self , event , publication ): \"\"\"add a publication to an event Arguments: event: the event we wan't to add a publication to publication: the publication to add \"\"\" logging . debug ( self . log + \"linked publication\" ) event . data [ 'obj' ][ 'data' ] = publication doi_base_url = \"https://doi.org/\" # todo event . data [ 'obj' ][ 'pid' ] = doi_base_url + publication [ 'doi' ] event . data [ 'obj' ][ 'alternative-id' ] = publication [ 'doi' ] event . set ( 'obj_id' , event . data [ 'obj' ][ 'pid' ]) get_publication_info ( self , doi ) get publication data for a doi using mongo and amba dbs Parameters: Name Type Description Default doi the doi for the publication we want required Source code in src/twitter_perculator.py def get_publication_info ( self , doi ): \"\"\"get publication data for a doi using mongo and amba dbs Arguments: doi: the doi for the publication we want \"\"\" publication = self . dao . get_publication ( doi ) if publication and isinstance ( publication , dict ): logging . debug ( publication ) logging . debug ( 'get publication from db' ) return publication if publication : logging . debug ( 'get publication from amba' ) publication = self . dao . save_publication ( publication ) return publication return { 'doi' : doi } on_message ( self , json_msg ) either link a event to a publication or add doi to it and mark it unknown to add the publication finder topic Parameters: Name Type Description Default json_msg json message representing a event required Source code in src/twitter_perculator.py def on_message ( self , json_msg ): \"\"\"either link a event to a publication or add doi to it and mark it unknown to add the publication finder topic Arguments: json_msg: json message representing a event \"\"\" if not self . dao : self . dao = DAO () e = Event () e . from_json ( json_msg ) e . data [ 'obj' ][ 'data' ] = {} if e . get ( 'source_id' ) == 'twitter' : # logging.warning(self.log + \"on message twitter perculator\") if 'id' in e . data [ 'subj' ][ 'data' ]: # logging.warning(self.log + e.data['subj']['data']['id']) # we use the id for mongo e . data [ 'subj' ][ 'data' ][ '_id' ] = e . data [ 'subj' ][ 'data' ] . pop ( 'id' ) # move matching rules to tweet self e . data [ 'subj' ][ 'data' ][ 'matching_rules' ] = e . data [ 'subj' ][ 'data' ][ 'matching_rules' ] running = False # check for doi recognition on tweet self doi = doi_resolver . url_doi_check ( e . data [ 'subj' ][ 'data' ]) if doi is not False : self . update_event ( e , doi ) # check if we have the conversation_id in our db # where discussionData.subjId == conversation_id # check the includes object for the original tweet url elif 'tweets' in e . data [ 'subj' ][ 'data' ][ 'includes' ]: # logging.warning('tweets') for tweet in e . data [ 'subj' ][ 'data' ][ 'includes' ][ 'tweets' ]: doi = doi_resolver . url_doi_check ( tweet ) # logging.warning('doi 2 ' + str(doi)) if doi is not False : # use first doi we get self . update_event ( e , doi ) break logging . debug ( self . log + e . data [ 'subj' ][ 'data' ][ '_id' ] + \" no doi\" ) logging . warning ( e . data [ 'subj' ][ 'data' ][ 'includes' ]) else : logging . debug ( self . log + e . data [ 'subj' ][ 'data' ][ '_id' ] + \" no doi\" ) logging . warning ( e . data [ 'subj' ][ 'data' ]) else : logging . debug ( 'no id' ) start ( i = 0 ) staticmethod start the consumer Source code in src/twitter_perculator.py @staticmethod def start ( i = 0 ): \"\"\"start the consumer \"\"\" tp = TwitterPerculator ( i ) logging . debug ( TwitterPerculator . log + 'Start %s ' % str ( i )) tp . consume () update_event ( self , event , doi ) update the event either with publication or just with doi and set the state accordingly Source code in src/twitter_perculator.py def update_event ( self , event , doi ): \"\"\"update the event either with publication or just with doi and set the state accordingly \"\"\" event . data [ 'obj' ][ 'data' ][ 'doi' ] = doi publication = self . get_publication_info ( doi ) if publication and isinstance ( publication , dict ) and 'title' in publication : self . add_publication ( event , publication ) logging . warning ( 'linked' ) event . set ( 'state' , 'linked' ) else : self . add_publication ( event , { 'doi' : doi }) logging . warning ( 'unknown' ) event . set ( 'state' , 'unknown' ) self . publish ( event )","title":"twitter perculator"},{"location":"twitter_perculator_ref/#twitter_perculator.TwitterPerculator","text":"link events to doi and if possible add a publication to it","title":"TwitterPerculator"},{"location":"twitter_perculator_ref/#twitter_perculator.TwitterPerculator.add_publication","text":"add a publication to an event Parameters: Name Type Description Default event the event we wan't to add a publication to required publication the publication to add required Source code in src/twitter_perculator.py def add_publication ( self , event , publication ): \"\"\"add a publication to an event Arguments: event: the event we wan't to add a publication to publication: the publication to add \"\"\" logging . debug ( self . log + \"linked publication\" ) event . data [ 'obj' ][ 'data' ] = publication doi_base_url = \"https://doi.org/\" # todo event . data [ 'obj' ][ 'pid' ] = doi_base_url + publication [ 'doi' ] event . data [ 'obj' ][ 'alternative-id' ] = publication [ 'doi' ] event . set ( 'obj_id' , event . data [ 'obj' ][ 'pid' ])","title":"add_publication()"},{"location":"twitter_perculator_ref/#twitter_perculator.TwitterPerculator.get_publication_info","text":"get publication data for a doi using mongo and amba dbs Parameters: Name Type Description Default doi the doi for the publication we want required Source code in src/twitter_perculator.py def get_publication_info ( self , doi ): \"\"\"get publication data for a doi using mongo and amba dbs Arguments: doi: the doi for the publication we want \"\"\" publication = self . dao . get_publication ( doi ) if publication and isinstance ( publication , dict ): logging . debug ( publication ) logging . debug ( 'get publication from db' ) return publication if publication : logging . debug ( 'get publication from amba' ) publication = self . dao . save_publication ( publication ) return publication return { 'doi' : doi }","title":"get_publication_info()"},{"location":"twitter_perculator_ref/#twitter_perculator.TwitterPerculator.on_message","text":"either link a event to a publication or add doi to it and mark it unknown to add the publication finder topic Parameters: Name Type Description Default json_msg json message representing a event required Source code in src/twitter_perculator.py def on_message ( self , json_msg ): \"\"\"either link a event to a publication or add doi to it and mark it unknown to add the publication finder topic Arguments: json_msg: json message representing a event \"\"\" if not self . dao : self . dao = DAO () e = Event () e . from_json ( json_msg ) e . data [ 'obj' ][ 'data' ] = {} if e . get ( 'source_id' ) == 'twitter' : # logging.warning(self.log + \"on message twitter perculator\") if 'id' in e . data [ 'subj' ][ 'data' ]: # logging.warning(self.log + e.data['subj']['data']['id']) # we use the id for mongo e . data [ 'subj' ][ 'data' ][ '_id' ] = e . data [ 'subj' ][ 'data' ] . pop ( 'id' ) # move matching rules to tweet self e . data [ 'subj' ][ 'data' ][ 'matching_rules' ] = e . data [ 'subj' ][ 'data' ][ 'matching_rules' ] running = False # check for doi recognition on tweet self doi = doi_resolver . url_doi_check ( e . data [ 'subj' ][ 'data' ]) if doi is not False : self . update_event ( e , doi ) # check if we have the conversation_id in our db # where discussionData.subjId == conversation_id # check the includes object for the original tweet url elif 'tweets' in e . data [ 'subj' ][ 'data' ][ 'includes' ]: # logging.warning('tweets') for tweet in e . data [ 'subj' ][ 'data' ][ 'includes' ][ 'tweets' ]: doi = doi_resolver . url_doi_check ( tweet ) # logging.warning('doi 2 ' + str(doi)) if doi is not False : # use first doi we get self . update_event ( e , doi ) break logging . debug ( self . log + e . data [ 'subj' ][ 'data' ][ '_id' ] + \" no doi\" ) logging . warning ( e . data [ 'subj' ][ 'data' ][ 'includes' ]) else : logging . debug ( self . log + e . data [ 'subj' ][ 'data' ][ '_id' ] + \" no doi\" ) logging . warning ( e . data [ 'subj' ][ 'data' ]) else : logging . debug ( 'no id' )","title":"on_message()"},{"location":"twitter_perculator_ref/#twitter_perculator.TwitterPerculator.start","text":"start the consumer Source code in src/twitter_perculator.py @staticmethod def start ( i = 0 ): \"\"\"start the consumer \"\"\" tp = TwitterPerculator ( i ) logging . debug ( TwitterPerculator . log + 'Start %s ' % str ( i )) tp . consume ()","title":"start()"},{"location":"twitter_perculator_ref/#twitter_perculator.TwitterPerculator.update_event","text":"update the event either with publication or just with doi and set the state accordingly Source code in src/twitter_perculator.py def update_event ( self , event , doi ): \"\"\"update the event either with publication or just with doi and set the state accordingly \"\"\" event . data [ 'obj' ][ 'data' ][ 'doi' ] = doi publication = self . get_publication_info ( doi ) if publication and isinstance ( publication , dict ) and 'title' in publication : self . add_publication ( event , publication ) logging . warning ( 'linked' ) event . set ( 'state' , 'linked' ) else : self . add_publication ( event , { 'doi' : doi }) logging . warning ( 'unknown' ) event . set ( 'state' , 'unknown' ) self . publish ( event )","title":"update_event()"}]}