{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"amba-analysis-worker-perculator Tries to map events of amba-analysis-stream to publications","title":"Home"},{"location":"#amba-analysis-worker-perculator","text":"Tries to map events of amba-analysis-stream to publications","title":"amba-analysis-worker-perculator"},{"location":"doi_resolver_ref/","text":"Helper Class providing multiple static functions to extract a doi from a url check_doi_list_valid ( potential_dois ) check if a list of potential dois are valid and if so return the valid doi Parameters: Name Type Description Default potential_dois a list of dois that should be checked required Source code in src/doi_resolver.py def check_doi_list_valid ( potential_dois ): \"\"\"check if a list of potential dois are valid and if so return the valid doi Arguments: potential_dois: a list of dois that should be checked \"\"\" pdoi = '' for doi in potential_dois : if doi is not None and doi : pdoi = pdoi + doi + ',' pre = \"http://doi.org/doiRA/\" # todo rw = False if pdoi != '' : r = requests . get ( pre + pdoi ) json_response = r . json () for j in json_response : if 'RA' in j : rw = j [ 'DOI' ] return rw crossref_url_search ( url ) search the url in crossref eventdata to get a doi Parameters: Name Type Description Default url the url to get a doi for required Source code in src/doi_resolver.py def crossref_url_search ( url ): \"\"\"search the url in crossref eventdata to get a doi Arguments: url: the url to get a doi for \"\"\" r = requests . get ( \"https://api.eventdata.crossref.org/v1/events?rows=1&obj.url=\" + url ) if r . status_code == 200 : json_response = r . json () if 'status' in json_response : if json_response [ 'status' ] == 'ok' : # logging.debug(json_response) if 'message' in json_response : if 'events' in json_response [ 'message' ]: for event in json_response [ 'message' ][ 'events' ]: return event [ 'obj_id' ][ 16 :] # https://doi.org/ -> 16 return False get_dois_regex ( regex , temp_doi ) find all occurrences of a given regex in a doi-string which ending is searched Parameters: Name Type Description Default regex the regex to look for required temp_doi the doi to use required Source code in src/doi_resolver.py def get_dois_regex ( regex , temp_doi ): \"\"\"find all occurrences of a given regex in a doi-string which ending is searched Arguments: regex: the regex to look for temp_doi: the doi to use \"\"\" if regex . search ( temp_doi ) is not None : # logging.debug(regex.search(temp_doi).group()) # logging.debug(regex.findall(temp_doi)) return regex . findall ( temp_doi )[ 0 ] get_filtered_dois_from_meta ( potential_dois ) check potential dois from meta tags for dois to extract them in case they have a full url Parameters: Name Type Description Default potential_dois the dois to filter required Source code in src/doi_resolver.py def get_filtered_dois_from_meta ( potential_dois ): \"\"\"check potential dois from meta tags for dois to extract them in case they have a full url Arguments: potential_dois: the dois to filter \"\"\" result = set ([]) for t in potential_dois : result . add ( t . replace ( 'doi:' , '' )) doi_re = re . compile ( \"(10 \\\\ . \\\\ d{4,9}(?:/| %2F | %2f )[^ \\\\ s]+)\" ) r = set ([]) for t in result : if doi_re . search ( t ) is not None : r . add ( t ) return r get_lxml ( page ) use lxml to search for meta tags that could contain the doi Parameters: Name Type Description Default page the page to search required Source code in src/doi_resolver.py def get_lxml ( page ): \"\"\"use lxml to search for meta tags that could contain the doi Arguments: page: the page to search \"\"\" content = html . fromstring ( page . content ) result = set ([]) # todo not only head but also body for meta in content . xpath ( '//html//meta' ): for name , value in sorted ( meta . items ()): # logging.debug(name)DC.Identifier if value . strip () . lower () in [ 'citation_doi' , 'dc.identifier' , 'evt-doipage' , 'news_doi' , 'citation_doi' ]: # logging.debug(meta.get('content')) result . add ( meta . get ( 'content' )) return result get_potential_dois_from_text ( text ) use multiple different regexes to get a list of potential dois, it uses a very generic regex first which only checks for the bare minimum start to the end of line, this result will than be searched for possible endings generating possible dois the result is a set and will likely contain unvalid dois Parameters: Name Type Description Default text the text which should be checked required Source code in src/doi_resolver.py def get_potential_dois_from_text ( text ): \"\"\"use multiple different regexes to get a list of potential dois, it uses a very generic regex first which only checks for the bare minimum start to the end of line, this result will than be searched for possible endings generating possible dois the result is a set and will likely contain unvalid dois Arguments: text: the text which should be checked \"\"\" doi_re = re . compile ( \"(10 \\\\ . \\\\ d{4,9}(?:/| %2F | %2f )[^ \\\\ s]+)\" ) last_slash = re . compile ( \"^(10 \\\\ . \\\\ d+/.*)/.*$\" ) first_slash = re . compile ( \"^(10 \\\\ . \\\\ d+/.*?)/.*$\" ) semicolon = re . compile ( \"^(10 \\\\ . \\\\ d+/.*);.*$\" ) hashchar = re . compile ( \"^(10 \\\\ . \\\\ d+/.*?)#.*$\" ) question_mark = re . compile ( \"^(10 \\\\ . \\\\ d+/.*?) \\\\ ?.*$\" ) amp_mark = re . compile ( \"^(10 \\\\ . \\\\ d+/.*?)&.*$\" ) v1 = re . compile ( \"^(10 \\\\ . \\\\ d+/.*)v1*$\" ) # biorxiv, make if v\\\\d+ (v and digit v1,v2,v3 result = set ([]) if doi_re . search ( text ) is not None : temp_doi = doi_re . search ( text ) . group () # logging.debug(doi_re.findall(ur)) # logging.debug(temp_doi) result . add ( temp_doi ) result . add ( get_dois_regex ( last_slash , temp_doi )) result . add ( get_dois_regex ( first_slash , temp_doi )) result . add ( get_dois_regex ( semicolon , temp_doi )) result . add ( get_dois_regex ( hashchar , temp_doi )) result . add ( get_dois_regex ( question_mark , temp_doi )) result . add ( get_dois_regex ( amp_mark , temp_doi )) result . add ( get_dois_regex ( v1 , temp_doi )) return result get_response ( url , s ) get a response from a given url using a given session s, a session can be used for headers, this function is cached up to 100 elements !!! arguments url: the url to get s: the session to use Source code in src/doi_resolver.py def get_response ( url , s ): \"\"\"get a response from a given url using a given session s, a session can be used for headers, this function is cached up to 100 elements Arguments: url: the url to get s: the session to use \"\"\" return s . get ( url ) link_url ( url ) link a url to a valid doi, it will try to get potential dois using multiple regex and than check if their are valid and than return the doi it uses multiple methods to search for the doi, this function is cached up to 500 elements !!! arguments url: the url to get s: the session to use Source code in src/doi_resolver.py @lru_cache ( maxsize = 10000 ) def link_url ( url ): \"\"\"link a url to a valid doi, it will try to get potential dois using multiple regex and than check if their are valid and than return the doi it uses multiple methods to search for the doi, this function is cached up to 500 elements Arguments: url: the url to get s: the session to use \"\"\" logging . debug ( url ) # check if the url contains the doi doi = check_doi_list_valid ( get_potential_dois_from_text ( url )) if doi : logging . debug ( 'url' ) return doi s = Session () # get the response for the provided url headers = { 'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' , 'Pragma' : 'no-cache' } s . headers . update ( headers ) r = get_response ( url , s ) # check if the doi is in any meta tag pot_doi = get_lxml ( r ) doi = check_doi_list_valid ( get_filtered_dois_from_meta ( pot_doi )) if doi and doi != set ([]): logging . debug ( 'meta' ) return doi # check if crossref knows this url and returns the doi doi = crossref_url_search ( url ) if doi : logging . debug ( 'crossref' ) return doi # do a fulltext search of the url doi = check_doi_list_valid ( search_fulltext ( r )) if doi : logging . debug ( 'fulltext' ) return doi return False search_fulltext ( r ) search the fulltext of an response Parameters: Name Type Description Default r the response we want to search required Source code in src/doi_resolver.py def search_fulltext ( r ): \"\"\"search the fulltext of an response Arguments: r: the response we want to search \"\"\" return get_potential_dois_from_text ( r . text ) url_doi_check ( data ) check data for urls, get first url in urls, prefer expanded_url !!! arguments data: data to get url from Source code in src/doi_resolver.py def url_doi_check ( data ): \"\"\"check data for urls, get first url in urls, prefer expanded_url Arguments: data: data to get url from \"\"\" doi_data = False if 'entities' in data : if 'urls' in data [ 'entities' ]: for url in data [ 'entities' ][ 'urls' ]: if doi_data is False and 'expanded_url' in url : doi_data = link_url ( url [ 'expanded_url' ]) if doi_data is False and 'unwound_url' in url : doi_data = link_url ( url [ 'unwound_url' ]) if doi_data is not False : return doi_data return doi_data","title":"doi resolver"},{"location":"doi_resolver_ref/#doi_resolver.check_doi_list_valid","text":"check if a list of potential dois are valid and if so return the valid doi Parameters: Name Type Description Default potential_dois a list of dois that should be checked required Source code in src/doi_resolver.py def check_doi_list_valid ( potential_dois ): \"\"\"check if a list of potential dois are valid and if so return the valid doi Arguments: potential_dois: a list of dois that should be checked \"\"\" pdoi = '' for doi in potential_dois : if doi is not None and doi : pdoi = pdoi + doi + ',' pre = \"http://doi.org/doiRA/\" # todo rw = False if pdoi != '' : r = requests . get ( pre + pdoi ) json_response = r . json () for j in json_response : if 'RA' in j : rw = j [ 'DOI' ] return rw","title":"check_doi_list_valid()"},{"location":"doi_resolver_ref/#doi_resolver.crossref_url_search","text":"search the url in crossref eventdata to get a doi Parameters: Name Type Description Default url the url to get a doi for required Source code in src/doi_resolver.py def crossref_url_search ( url ): \"\"\"search the url in crossref eventdata to get a doi Arguments: url: the url to get a doi for \"\"\" r = requests . get ( \"https://api.eventdata.crossref.org/v1/events?rows=1&obj.url=\" + url ) if r . status_code == 200 : json_response = r . json () if 'status' in json_response : if json_response [ 'status' ] == 'ok' : # logging.debug(json_response) if 'message' in json_response : if 'events' in json_response [ 'message' ]: for event in json_response [ 'message' ][ 'events' ]: return event [ 'obj_id' ][ 16 :] # https://doi.org/ -> 16 return False","title":"crossref_url_search()"},{"location":"doi_resolver_ref/#doi_resolver.get_dois_regex","text":"find all occurrences of a given regex in a doi-string which ending is searched Parameters: Name Type Description Default regex the regex to look for required temp_doi the doi to use required Source code in src/doi_resolver.py def get_dois_regex ( regex , temp_doi ): \"\"\"find all occurrences of a given regex in a doi-string which ending is searched Arguments: regex: the regex to look for temp_doi: the doi to use \"\"\" if regex . search ( temp_doi ) is not None : # logging.debug(regex.search(temp_doi).group()) # logging.debug(regex.findall(temp_doi)) return regex . findall ( temp_doi )[ 0 ]","title":"get_dois_regex()"},{"location":"doi_resolver_ref/#doi_resolver.get_filtered_dois_from_meta","text":"check potential dois from meta tags for dois to extract them in case they have a full url Parameters: Name Type Description Default potential_dois the dois to filter required Source code in src/doi_resolver.py def get_filtered_dois_from_meta ( potential_dois ): \"\"\"check potential dois from meta tags for dois to extract them in case they have a full url Arguments: potential_dois: the dois to filter \"\"\" result = set ([]) for t in potential_dois : result . add ( t . replace ( 'doi:' , '' )) doi_re = re . compile ( \"(10 \\\\ . \\\\ d{4,9}(?:/| %2F | %2f )[^ \\\\ s]+)\" ) r = set ([]) for t in result : if doi_re . search ( t ) is not None : r . add ( t ) return r","title":"get_filtered_dois_from_meta()"},{"location":"doi_resolver_ref/#doi_resolver.get_lxml","text":"use lxml to search for meta tags that could contain the doi Parameters: Name Type Description Default page the page to search required Source code in src/doi_resolver.py def get_lxml ( page ): \"\"\"use lxml to search for meta tags that could contain the doi Arguments: page: the page to search \"\"\" content = html . fromstring ( page . content ) result = set ([]) # todo not only head but also body for meta in content . xpath ( '//html//meta' ): for name , value in sorted ( meta . items ()): # logging.debug(name)DC.Identifier if value . strip () . lower () in [ 'citation_doi' , 'dc.identifier' , 'evt-doipage' , 'news_doi' , 'citation_doi' ]: # logging.debug(meta.get('content')) result . add ( meta . get ( 'content' )) return result","title":"get_lxml()"},{"location":"doi_resolver_ref/#doi_resolver.get_potential_dois_from_text","text":"use multiple different regexes to get a list of potential dois, it uses a very generic regex first which only checks for the bare minimum start to the end of line, this result will than be searched for possible endings generating possible dois the result is a set and will likely contain unvalid dois Parameters: Name Type Description Default text the text which should be checked required Source code in src/doi_resolver.py def get_potential_dois_from_text ( text ): \"\"\"use multiple different regexes to get a list of potential dois, it uses a very generic regex first which only checks for the bare minimum start to the end of line, this result will than be searched for possible endings generating possible dois the result is a set and will likely contain unvalid dois Arguments: text: the text which should be checked \"\"\" doi_re = re . compile ( \"(10 \\\\ . \\\\ d{4,9}(?:/| %2F | %2f )[^ \\\\ s]+)\" ) last_slash = re . compile ( \"^(10 \\\\ . \\\\ d+/.*)/.*$\" ) first_slash = re . compile ( \"^(10 \\\\ . \\\\ d+/.*?)/.*$\" ) semicolon = re . compile ( \"^(10 \\\\ . \\\\ d+/.*);.*$\" ) hashchar = re . compile ( \"^(10 \\\\ . \\\\ d+/.*?)#.*$\" ) question_mark = re . compile ( \"^(10 \\\\ . \\\\ d+/.*?) \\\\ ?.*$\" ) amp_mark = re . compile ( \"^(10 \\\\ . \\\\ d+/.*?)&.*$\" ) v1 = re . compile ( \"^(10 \\\\ . \\\\ d+/.*)v1*$\" ) # biorxiv, make if v\\\\d+ (v and digit v1,v2,v3 result = set ([]) if doi_re . search ( text ) is not None : temp_doi = doi_re . search ( text ) . group () # logging.debug(doi_re.findall(ur)) # logging.debug(temp_doi) result . add ( temp_doi ) result . add ( get_dois_regex ( last_slash , temp_doi )) result . add ( get_dois_regex ( first_slash , temp_doi )) result . add ( get_dois_regex ( semicolon , temp_doi )) result . add ( get_dois_regex ( hashchar , temp_doi )) result . add ( get_dois_regex ( question_mark , temp_doi )) result . add ( get_dois_regex ( amp_mark , temp_doi )) result . add ( get_dois_regex ( v1 , temp_doi )) return result","title":"get_potential_dois_from_text()"},{"location":"doi_resolver_ref/#doi_resolver.get_response","text":"get a response from a given url using a given session s, a session can be used for headers, this function is cached up to 100 elements !!! arguments url: the url to get s: the session to use Source code in src/doi_resolver.py def get_response ( url , s ): \"\"\"get a response from a given url using a given session s, a session can be used for headers, this function is cached up to 100 elements Arguments: url: the url to get s: the session to use \"\"\" return s . get ( url )","title":"get_response()"},{"location":"doi_resolver_ref/#doi_resolver.link_url","text":"link a url to a valid doi, it will try to get potential dois using multiple regex and than check if their are valid and than return the doi it uses multiple methods to search for the doi, this function is cached up to 500 elements !!! arguments url: the url to get s: the session to use Source code in src/doi_resolver.py @lru_cache ( maxsize = 10000 ) def link_url ( url ): \"\"\"link a url to a valid doi, it will try to get potential dois using multiple regex and than check if their are valid and than return the doi it uses multiple methods to search for the doi, this function is cached up to 500 elements Arguments: url: the url to get s: the session to use \"\"\" logging . debug ( url ) # check if the url contains the doi doi = check_doi_list_valid ( get_potential_dois_from_text ( url )) if doi : logging . debug ( 'url' ) return doi s = Session () # get the response for the provided url headers = { 'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36' , 'Pragma' : 'no-cache' } s . headers . update ( headers ) r = get_response ( url , s ) # check if the doi is in any meta tag pot_doi = get_lxml ( r ) doi = check_doi_list_valid ( get_filtered_dois_from_meta ( pot_doi )) if doi and doi != set ([]): logging . debug ( 'meta' ) return doi # check if crossref knows this url and returns the doi doi = crossref_url_search ( url ) if doi : logging . debug ( 'crossref' ) return doi # do a fulltext search of the url doi = check_doi_list_valid ( search_fulltext ( r )) if doi : logging . debug ( 'fulltext' ) return doi return False","title":"link_url()"},{"location":"doi_resolver_ref/#doi_resolver.search_fulltext","text":"search the fulltext of an response Parameters: Name Type Description Default r the response we want to search required Source code in src/doi_resolver.py def search_fulltext ( r ): \"\"\"search the fulltext of an response Arguments: r: the response we want to search \"\"\" return get_potential_dois_from_text ( r . text )","title":"search_fulltext()"},{"location":"doi_resolver_ref/#doi_resolver.url_doi_check","text":"check data for urls, get first url in urls, prefer expanded_url !!! arguments data: data to get url from Source code in src/doi_resolver.py def url_doi_check ( data ): \"\"\"check data for urls, get first url in urls, prefer expanded_url Arguments: data: data to get url from \"\"\" doi_data = False if 'entities' in data : if 'urls' in data [ 'entities' ]: for url in data [ 'entities' ][ 'urls' ]: if doi_data is False and 'expanded_url' in url : doi_data = link_url ( url [ 'expanded_url' ]) if doi_data is False and 'unwound_url' in url : doi_data = link_url ( url [ 'unwound_url' ]) if doi_data is not False : return doi_data return doi_data","title":"url_doi_check()"},{"location":"twitter_perculator_ref/","text":"TwitterPerculator link events to doi and if possible add a publication to it add_publication ( self , event , publication ) add a publication to an event Parameters: Name Type Description Default event the event we wan't to add a publication to required publication the publication to add required Source code in src/twitter_perculator.py def add_publication ( self , event , publication ): \"\"\"add a publication to an event Arguments: event: the event we wan't to add a publication to publication: the publication to add \"\"\" logging . warning ( self . log + \"linked publication\" ) event . data [ 'obj' ][ 'data' ] = publication doi_base_url = \"https://doi.org/\" # todo event . data [ 'obj' ][ 'pid' ] = doi_base_url + publication [ 'doi' ] event . data [ 'obj' ][ 'alternative-id' ] = publication [ 'doi' ] event . set ( 'obj_id' , event . data [ 'obj' ][ 'pid' ]) get_publication_info ( self , doi ) get publication data for a doi using mongo and amba dbs Parameters: Name Type Description Default doi the doi for the publication we want required Source code in src/twitter_perculator.py def get_publication_info ( self , doi ): \"\"\"get publication data for a doi using mongo and amba dbs Arguments: doi: the doi for the publication we want \"\"\" publication = get_publication_from_mongo ( self . collection , doi ) if publication : logging . debug ( 'get publication from mongo' ) return publication if not self . amba_client : self . prepare_amba_connection () publication = get_publication_from_amba ( self . amba_client , doi ) if publication : logging . debug ( 'get publication from amba' ) self . save_publication_to_mongo ( publication ) return publication return { 'doi' : doi } on_message ( self , json_msg ) the on message function to be implemented in own classes Parameters: Name Type Description Default json_msg the message to do stuff with required Source code in src/twitter_perculator.py def on_message ( self , json_msg ): if not self . mongo_client : self . prepare_mongo_connection () \"\"\"either link a event to a publication or add doi to it and mark it unknown to add the publication finder topic Arguments: json_msg: json message representing a event \"\"\" e = Event () e . from_json ( json_msg ) e . data [ 'obj' ][ 'data' ] = {} # todo check that source_id is twitter # json_msg = e.data['subj']['data'] # logging.warning(self.log + \"on message twitter perculator\") if 'id' in e . data [ 'subj' ][ 'data' ]: logging . warning ( self . log + e . data [ 'subj' ][ 'data' ][ 'id' ]) # we use the id for mongo e . data [ 'subj' ][ 'data' ][ '_id' ] = e . data [ 'subj' ][ 'data' ] . pop ( 'id' ) # move matching rules to tweet self e . data [ 'subj' ][ 'data' ][ 'matching_rules' ] = e . data [ 'subj' ][ 'data' ][ 'matching_rules' ] running = False # check for doi recognition on tweet self doi = doi_resolver . url_doi_check ( e . data [ 'subj' ][ 'data' ]) # logging.warning('doi 1 ' + str(doi)) if doi is not False : e . data [ 'obj' ][ 'data' ][ 'doi' ] = doi # e.data['doiTemp'] = doi publication = self . get_publication_info ( doi ) self . add_publication ( e , publication ) if 'title' in publication : e . set ( 'state' , 'linked' ) logging . warning ( 'publish linked message of doi ' ) else : e . set ( 'state' , 'unknown' ) self . publish ( e ) # check the includes object for the original tweet url elif 'tweets' in e . data [ 'subj' ][ 'data' ][ 'includes' ]: # logging.warning('tweets') for tweet in e . data [ 'subj' ][ 'data' ][ 'includes' ][ 'tweets' ]: doi = doi_resolver . url_doi_check ( tweet ) # logging.warning('doi 2 ' + str(doi)) if doi is not False : # use first doi we get # logging.warning(self.log + e.data['subj']['data']['_id'] + \" doi includes\") e . data [ 'obj' ][ 'data' ][ 'doi' ] = doi # e.data['doiTemp'] = doi publication = self . get_publication_info ( doi ) self . add_publication ( e , publication ) if 'title' in publication : e . set ( 'state' , 'linked' ) logging . warning ( 'publish linked message of doi in includes' ) else : e . set ( 'state' , 'unknown' ) self . publish ( e ) # publish_message(producer, parsed_topic_name, 'parsed', # json.dumps(json_msg['data'], indent=2).encode('utf-8')) break else : logging . warning ( self . log + e . data [ 'subj' ][ 'data' ][ '_id' ] + \" no doi\" ) self . save_not_perculated ( e ) else : logging . warning ( self . log + e . data [ 'subj' ][ 'data' ][ '_id' ] + \" no doi\" ) self . save_not_perculated ( e ) prepare_amba_connection ( self ) prepare the amba connection abd setup the client Source code in src/twitter_perculator.py def prepare_amba_connection ( self ): \"\"\"prepare the amba connection abd setup the client \"\"\" transport = AIOHTTPTransport ( url = self . config [ 'url' ]) self . amba_client = Client ( transport = transport , fetch_schema_from_transport = True ) prepare_mongo_connection ( self ) prepare mongo connection and setup the client Source code in src/twitter_perculator.py def prepare_mongo_connection ( self ): \"\"\"prepare mongo connection and setup the client \"\"\" self . mongo_client = pymongo . MongoClient ( host = self . config [ 'mongo_url' ], serverSelectionTimeoutMS = 3000 , # 3 second timeout username = \"root\" , password = \"example\" ) db = self . mongo_client [ self . config [ 'mongo_client' ]] self . collection = db [ self . config [ 'mongo_collection' ]] self . collectionFailed = db [ 'failed' ] # todo only debug? save_not_perculated ( self , event ) save thea not peruclated event for debug Parameters: Name Type Description Default event Event the event to save required Source code in src/twitter_perculator.py def save_not_perculated ( self , event : Event ): \"\"\" save thea not peruclated event for debug Arguments: event: the event to save \"\"\" logging . debug ( 'save publication to mongo' ) try : event . data [ '_id' ] = event . data [ 'id' ] self . collectionFailed . insert_one ( event . data ) except pymongo . errors . DuplicateKeyError : logging . warning ( \"MongoDB, not perculated event \" % event ) save_publication_to_mongo ( self , publication ) save the publication to our mongo to this allows faster access and to store calculated data right on it additional ist easier only to check one db use doi as id and key Parameters: Name Type Description Default publication the publication to save required Source code in src/twitter_perculator.py def save_publication_to_mongo ( self , publication ): \"\"\" save the publication to our mongo to this allows faster access and to store calculated data right on it additional ist easier only to check one db use doi as id and key Arguments: publication: the publication to save \"\"\" logging . debug ( 'save publication to mongo' ) try : # publication['_id'] = publication['id'] publication [ '_id' ] = publication [ 'doi' ] publication [ 'source' ] = 'amba' publication [ 'source-a' ] = 'perculator' # todo remove self . collection . insert_one ( publication ) except pymongo . errors . DuplicateKeyError : logging . warning ( \"MongoDB publication, Duplicate found, continue\" % publication ) get_publication_from_amba ( amba_client , doi ) get a publication from amba client using a collection and a doi. this is cached up to 100 Parameters: Name Type Description Default amba_client the amba_client to use required doi the doi required Source code in src/twitter_perculator.py def get_publication_from_amba ( amba_client , doi ): \"\"\"get a publication from amba client using a collection and a doi. this is cached up to 100 Arguments: amba_client: the amba_client to use doi: the doi \"\"\" query = gql ( \"\"\" query getPublication($doi: [String!]!) { publicationsByDoi(doi: $doi) { id, type, doi, abstract, pubDate, publisher, rank, citationCount, title, normalizedTitle, year, citations { id, type, doi, abstract, pubDate, publisher, rank, citationCount, title, normalizedTitle, year }, refs { id, type, doi, abstract, pubDate, publisher, rank, citationCount, title, normalizedTitle, year }, authors { id, name, normalizedName, pubCount, citationCount, rank }, fieldsOfStudy { score, name, normalizedName, level, rank, citationCount } } } \"\"\" ) # todo affiliation: Affiliation author # parents: [FieldOfStudy!] # children: [FieldOfStudy!] params = { \"doi\" : doi } result = amba_client . execute ( query , variable_values = params ) if 'publicationsByDoi' in result and len ( result [ 'publicationsByDoi' ]) > 0 : publication = result [ 'publicationsByDoi' ][ 0 ] return publication else : logging . warning ( 'unable to get data for doi: %s ' % doi ) return None get_publication_from_mongo ( collection , doi ) get a publication from mongo db using a collection and a doi. this is cached up to 100 !!! arguments collection: the collection to use doi: the doi Source code in src/twitter_perculator.py def get_publication_from_mongo ( collection , doi ): \"\"\"get a publication from mongo db using a collection and a doi. this is cached up to 100 Arguments: collection: the collection to use doi: the doi \"\"\" return collection . find_one ({ \"doi\" : doi })","title":"twitter perculator"},{"location":"twitter_perculator_ref/#twitter_perculator.TwitterPerculator","text":"link events to doi and if possible add a publication to it","title":"TwitterPerculator"},{"location":"twitter_perculator_ref/#twitter_perculator.TwitterPerculator.add_publication","text":"add a publication to an event Parameters: Name Type Description Default event the event we wan't to add a publication to required publication the publication to add required Source code in src/twitter_perculator.py def add_publication ( self , event , publication ): \"\"\"add a publication to an event Arguments: event: the event we wan't to add a publication to publication: the publication to add \"\"\" logging . warning ( self . log + \"linked publication\" ) event . data [ 'obj' ][ 'data' ] = publication doi_base_url = \"https://doi.org/\" # todo event . data [ 'obj' ][ 'pid' ] = doi_base_url + publication [ 'doi' ] event . data [ 'obj' ][ 'alternative-id' ] = publication [ 'doi' ] event . set ( 'obj_id' , event . data [ 'obj' ][ 'pid' ])","title":"add_publication()"},{"location":"twitter_perculator_ref/#twitter_perculator.TwitterPerculator.get_publication_info","text":"get publication data for a doi using mongo and amba dbs Parameters: Name Type Description Default doi the doi for the publication we want required Source code in src/twitter_perculator.py def get_publication_info ( self , doi ): \"\"\"get publication data for a doi using mongo and amba dbs Arguments: doi: the doi for the publication we want \"\"\" publication = get_publication_from_mongo ( self . collection , doi ) if publication : logging . debug ( 'get publication from mongo' ) return publication if not self . amba_client : self . prepare_amba_connection () publication = get_publication_from_amba ( self . amba_client , doi ) if publication : logging . debug ( 'get publication from amba' ) self . save_publication_to_mongo ( publication ) return publication return { 'doi' : doi }","title":"get_publication_info()"},{"location":"twitter_perculator_ref/#twitter_perculator.TwitterPerculator.on_message","text":"the on message function to be implemented in own classes Parameters: Name Type Description Default json_msg the message to do stuff with required Source code in src/twitter_perculator.py def on_message ( self , json_msg ): if not self . mongo_client : self . prepare_mongo_connection () \"\"\"either link a event to a publication or add doi to it and mark it unknown to add the publication finder topic Arguments: json_msg: json message representing a event \"\"\" e = Event () e . from_json ( json_msg ) e . data [ 'obj' ][ 'data' ] = {} # todo check that source_id is twitter # json_msg = e.data['subj']['data'] # logging.warning(self.log + \"on message twitter perculator\") if 'id' in e . data [ 'subj' ][ 'data' ]: logging . warning ( self . log + e . data [ 'subj' ][ 'data' ][ 'id' ]) # we use the id for mongo e . data [ 'subj' ][ 'data' ][ '_id' ] = e . data [ 'subj' ][ 'data' ] . pop ( 'id' ) # move matching rules to tweet self e . data [ 'subj' ][ 'data' ][ 'matching_rules' ] = e . data [ 'subj' ][ 'data' ][ 'matching_rules' ] running = False # check for doi recognition on tweet self doi = doi_resolver . url_doi_check ( e . data [ 'subj' ][ 'data' ]) # logging.warning('doi 1 ' + str(doi)) if doi is not False : e . data [ 'obj' ][ 'data' ][ 'doi' ] = doi # e.data['doiTemp'] = doi publication = self . get_publication_info ( doi ) self . add_publication ( e , publication ) if 'title' in publication : e . set ( 'state' , 'linked' ) logging . warning ( 'publish linked message of doi ' ) else : e . set ( 'state' , 'unknown' ) self . publish ( e ) # check the includes object for the original tweet url elif 'tweets' in e . data [ 'subj' ][ 'data' ][ 'includes' ]: # logging.warning('tweets') for tweet in e . data [ 'subj' ][ 'data' ][ 'includes' ][ 'tweets' ]: doi = doi_resolver . url_doi_check ( tweet ) # logging.warning('doi 2 ' + str(doi)) if doi is not False : # use first doi we get # logging.warning(self.log + e.data['subj']['data']['_id'] + \" doi includes\") e . data [ 'obj' ][ 'data' ][ 'doi' ] = doi # e.data['doiTemp'] = doi publication = self . get_publication_info ( doi ) self . add_publication ( e , publication ) if 'title' in publication : e . set ( 'state' , 'linked' ) logging . warning ( 'publish linked message of doi in includes' ) else : e . set ( 'state' , 'unknown' ) self . publish ( e ) # publish_message(producer, parsed_topic_name, 'parsed', # json.dumps(json_msg['data'], indent=2).encode('utf-8')) break else : logging . warning ( self . log + e . data [ 'subj' ][ 'data' ][ '_id' ] + \" no doi\" ) self . save_not_perculated ( e ) else : logging . warning ( self . log + e . data [ 'subj' ][ 'data' ][ '_id' ] + \" no doi\" ) self . save_not_perculated ( e )","title":"on_message()"},{"location":"twitter_perculator_ref/#twitter_perculator.TwitterPerculator.prepare_amba_connection","text":"prepare the amba connection abd setup the client Source code in src/twitter_perculator.py def prepare_amba_connection ( self ): \"\"\"prepare the amba connection abd setup the client \"\"\" transport = AIOHTTPTransport ( url = self . config [ 'url' ]) self . amba_client = Client ( transport = transport , fetch_schema_from_transport = True )","title":"prepare_amba_connection()"},{"location":"twitter_perculator_ref/#twitter_perculator.TwitterPerculator.prepare_mongo_connection","text":"prepare mongo connection and setup the client Source code in src/twitter_perculator.py def prepare_mongo_connection ( self ): \"\"\"prepare mongo connection and setup the client \"\"\" self . mongo_client = pymongo . MongoClient ( host = self . config [ 'mongo_url' ], serverSelectionTimeoutMS = 3000 , # 3 second timeout username = \"root\" , password = \"example\" ) db = self . mongo_client [ self . config [ 'mongo_client' ]] self . collection = db [ self . config [ 'mongo_collection' ]] self . collectionFailed = db [ 'failed' ] # todo only debug?","title":"prepare_mongo_connection()"},{"location":"twitter_perculator_ref/#twitter_perculator.TwitterPerculator.save_not_perculated","text":"save thea not peruclated event for debug Parameters: Name Type Description Default event Event the event to save required Source code in src/twitter_perculator.py def save_not_perculated ( self , event : Event ): \"\"\" save thea not peruclated event for debug Arguments: event: the event to save \"\"\" logging . debug ( 'save publication to mongo' ) try : event . data [ '_id' ] = event . data [ 'id' ] self . collectionFailed . insert_one ( event . data ) except pymongo . errors . DuplicateKeyError : logging . warning ( \"MongoDB, not perculated event \" % event )","title":"save_not_perculated()"},{"location":"twitter_perculator_ref/#twitter_perculator.TwitterPerculator.save_publication_to_mongo","text":"save the publication to our mongo to this allows faster access and to store calculated data right on it additional ist easier only to check one db use doi as id and key Parameters: Name Type Description Default publication the publication to save required Source code in src/twitter_perculator.py def save_publication_to_mongo ( self , publication ): \"\"\" save the publication to our mongo to this allows faster access and to store calculated data right on it additional ist easier only to check one db use doi as id and key Arguments: publication: the publication to save \"\"\" logging . debug ( 'save publication to mongo' ) try : # publication['_id'] = publication['id'] publication [ '_id' ] = publication [ 'doi' ] publication [ 'source' ] = 'amba' publication [ 'source-a' ] = 'perculator' # todo remove self . collection . insert_one ( publication ) except pymongo . errors . DuplicateKeyError : logging . warning ( \"MongoDB publication, Duplicate found, continue\" % publication )","title":"save_publication_to_mongo()"},{"location":"twitter_perculator_ref/#twitter_perculator.get_publication_from_amba","text":"get a publication from amba client using a collection and a doi. this is cached up to 100 Parameters: Name Type Description Default amba_client the amba_client to use required doi the doi required Source code in src/twitter_perculator.py def get_publication_from_amba ( amba_client , doi ): \"\"\"get a publication from amba client using a collection and a doi. this is cached up to 100 Arguments: amba_client: the amba_client to use doi: the doi \"\"\" query = gql ( \"\"\" query getPublication($doi: [String!]!) { publicationsByDoi(doi: $doi) { id, type, doi, abstract, pubDate, publisher, rank, citationCount, title, normalizedTitle, year, citations { id, type, doi, abstract, pubDate, publisher, rank, citationCount, title, normalizedTitle, year }, refs { id, type, doi, abstract, pubDate, publisher, rank, citationCount, title, normalizedTitle, year }, authors { id, name, normalizedName, pubCount, citationCount, rank }, fieldsOfStudy { score, name, normalizedName, level, rank, citationCount } } } \"\"\" ) # todo affiliation: Affiliation author # parents: [FieldOfStudy!] # children: [FieldOfStudy!] params = { \"doi\" : doi } result = amba_client . execute ( query , variable_values = params ) if 'publicationsByDoi' in result and len ( result [ 'publicationsByDoi' ]) > 0 : publication = result [ 'publicationsByDoi' ][ 0 ] return publication else : logging . warning ( 'unable to get data for doi: %s ' % doi ) return None","title":"get_publication_from_amba()"},{"location":"twitter_perculator_ref/#twitter_perculator.get_publication_from_mongo","text":"get a publication from mongo db using a collection and a doi. this is cached up to 100 !!! arguments collection: the collection to use doi: the doi Source code in src/twitter_perculator.py def get_publication_from_mongo ( collection , doi ): \"\"\"get a publication from mongo db using a collection and a doi. this is cached up to 100 Arguments: collection: the collection to use doi: the doi \"\"\" return collection . find_one ({ \"doi\" : doi })","title":"get_publication_from_mongo()"},{"location":"twitter_supervisor_ref/","text":"Supervisor supervisor class __init__ ( self ) special init Source code in src/twitter_supervisor.py def __init__ ( self ): \"\"\"init\"\"\" self . running = True signal . signal ( signal . SIGINT , self . stop ) signal . signal ( signal . SIGTERM , self . stop ) # need main ( self ) setup logging and workers Source code in src/twitter_supervisor.py def main ( self ): \"\"\"setup logging and workers\"\"\" format = \" %(asctime)s : %(message)s \" logging . basicConfig ( format = format , level = logging . INFO , datefmt = \"%H:%M:%S\" ) self . workers = list () stop ( self , signum ) stop Source code in src/twitter_supervisor.py def stop ( self , signum ): \"\"\"stop\"\"\" logging . warning ( \"Supervisor :\" + signum ) self . running = False stopWorkers ( self ) stop worker Source code in src/twitter_supervisor.py def stopWorkers ( self ): \"\"\"stop worker\"\"\" logging . warning ( \"Supervisor : close threads.\" ) for worker in self . workers : worker . kill ()","title":"twitter supervisor"},{"location":"twitter_supervisor_ref/#twitter_supervisor.Supervisor","text":"supervisor class","title":"Supervisor"},{"location":"twitter_supervisor_ref/#twitter_supervisor.Supervisor.__init__","text":"init Source code in src/twitter_supervisor.py def __init__ ( self ): \"\"\"init\"\"\" self . running = True signal . signal ( signal . SIGINT , self . stop ) signal . signal ( signal . SIGTERM , self . stop ) # need","title":"__init__()"},{"location":"twitter_supervisor_ref/#twitter_supervisor.Supervisor.main","text":"setup logging and workers Source code in src/twitter_supervisor.py def main ( self ): \"\"\"setup logging and workers\"\"\" format = \" %(asctime)s : %(message)s \" logging . basicConfig ( format = format , level = logging . INFO , datefmt = \"%H:%M:%S\" ) self . workers = list ()","title":"main()"},{"location":"twitter_supervisor_ref/#twitter_supervisor.Supervisor.stop","text":"stop Source code in src/twitter_supervisor.py def stop ( self , signum ): \"\"\"stop\"\"\" logging . warning ( \"Supervisor :\" + signum ) self . running = False","title":"stop()"},{"location":"twitter_supervisor_ref/#twitter_supervisor.Supervisor.stopWorkers","text":"stop worker Source code in src/twitter_supervisor.py def stopWorkers ( self ): \"\"\"stop worker\"\"\" logging . warning ( \"Supervisor : close threads.\" ) for worker in self . workers : worker . kill ()","title":"stopWorkers()"}]}